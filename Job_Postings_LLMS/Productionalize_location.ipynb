{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff685ef-fdd5-4b5d-8dde-b1440427b7dc",
   "metadata": {},
   "source": [
    "# README"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc5c021-2ad1-418e-9539-812c2a6709c1",
   "metadata": {},
   "source": [
    "This File is used to take the Location NER pytorch model to onnx model and then to optomized tensorrt model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85840eac-6568-47ae-930f-68d86cf1b025",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0287dbee-2853-46b2-b134-d8671f79c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, DistilBertTokenizer, AutoModelForTokenClassification\n",
    "import onnx\n",
    "import tensorrt as trt\n",
    "import onnxruntime as ort\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2992134f-4c72-4435-99a6-c5c4851b4d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/connor/Documents/jobs/jobs/Location_Model\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e0579-d7d7-427b-8e69-799fe7b8b398",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f478c6e-6d60-4a07-9bb5-2f00af63fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ner_label_mappings(token_labels):\n",
    "    label2id = {'O': 0}\n",
    "    id2label = {0: 'O'}\n",
    "    index = 1  \n",
    "    for label in token_labels:\n",
    "        for prefix in ['B-', 'I-']:\n",
    "            current_label = f\"{prefix}{label.upper()}\"\n",
    "            label2id[current_label] = index\n",
    "            id2label[index] = current_label\n",
    "            index += 1\n",
    "    print(f\"Num Labels: {len(label2id.keys())}\")\n",
    "    print(f\"label2id: {label2id}\")\n",
    "    print(f\"id2label: {id2label}\")\n",
    "    return label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf309ffd-908a-4539-bf8e-346f884b14f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Labels: 9\n",
      "label2id: {'O': 0, 'B-CITY': 1, 'I-CITY': 2, 'B-STATE': 3, 'I-STATE': 4, 'B-COUNTRY': 5, 'I-COUNTRY': 6, 'B-REMOTE': 7, 'I-REMOTE': 8}\n",
      "id2label: {0: 'O', 1: 'B-CITY', 2: 'I-CITY', 3: 'B-STATE', 4: 'I-STATE', 5: 'B-COUNTRY', 6: 'I-COUNTRY', 7: 'B-REMOTE', 8: 'I-REMOTE'}\n"
     ]
    }
   ],
   "source": [
    "# Create Label Mappings\n",
    "token_labels = ['city', 'state', 'country', 'remote']\n",
    "label2id, id2label = create_ner_label_mappings(token_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee4a7cc-d911-4518-bb02-d485fd0fe1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory File path declaration \n",
    "out_put_dir = \"./location_bert_v2\"\n",
    "tokenizer_out_put_dir = out_put_dir+\"_tokenizer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea60899-2470-4eeb-9b0b-bb546bd7ae29",
   "metadata": {},
   "source": [
    "# Create production model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca7b05-31df-4113-893c-e4bedb6bc9a3",
   "metadata": {},
   "source": [
    "## Tokenizer and Model Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce76a18-7a1a-49b1-b20d-76c2ad14fddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connor/anaconda3/envs/job_11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loaded_tokenizer = BertTokenizer.from_pretrained(tokenizer_out_put_dir)\n",
    "with open(tokenizer_out_put_dir + \"/tokenizer_config.json\", \"r\") as f:\n",
    "    tokenizer_config = json.load(f)\n",
    "\n",
    "max_length = tokenizer_config.get('max_length', 128)\n",
    "truncation = tokenizer_config.get('truncation', True)\n",
    "padding = tokenizer_config.get('padding', True)\n",
    "return_offsets_mapping = tokenizer_config.get('return_offsets_mapping', True)\n",
    "is_split_into_words = tokenizer_config.get('is_split_into_words', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58d56f61-c865-45b6-bcd7-99ca51278a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForTokenClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = AutoModelForTokenClassification.from_pretrained(out_put_dir)\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f178572-9355-4a5e-a31f-305baa440ec9",
   "metadata": {},
   "source": [
    "## Data Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d61755-6312-47f4-a5af-6ae96a19adb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_location</th>\n",
       "      <th>fmt_raw_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>North Bethesda, MD, Lexington, KY, Remote</td>\n",
       "      <td>north bethesda, md, lexington, ky, remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remote - United Kingdom</td>\n",
       "      <td>remote - united kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remote</td>\n",
       "      <td>remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US - San Francisco</td>\n",
       "      <td>us - san francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remote, USA</td>\n",
       "      <td>remote, usa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                raw_location  \\\n",
       "0  North Bethesda, MD, Lexington, KY, Remote   \n",
       "1                   Remote - United Kingdom    \n",
       "2                                     Remote   \n",
       "3                         US - San Francisco   \n",
       "4                                Remote, USA   \n",
       "\n",
       "                            fmt_raw_location  \n",
       "0  north bethesda, md, lexington, ky, remote  \n",
       "1                    remote - united kingdom  \n",
       "2                                     remote  \n",
       "3                         us - san francisco  \n",
       "4                                remote, usa  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.read_csv('location_data/all_data_fmt_location.csv')\n",
    "all_df.drop(axis=1, inplace=True, columns=['id','title','description'])\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d4ecf-4489-4eef-acc9-b221d97e3c43",
   "metadata": {},
   "source": [
    "## Create Input Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f158e15d-e514-4170-9943-7ac9095c5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = all_df.iloc[0]['fmt_raw_location']\n",
    "inputs = loaded_tokenizer(\n",
    "    text,\n",
    "    max_length=max_length,\n",
    "    truncation=truncation,\n",
    "    padding=padding,\n",
    "    return_tensors=\"pt\",\n",
    "    is_split_into_words=is_split_into_words,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd7286a-0252-412c-932a-c6f7759eac20",
   "metadata": {},
   "source": [
    "## Create Onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0adfbece-a2e3-4704-8920-c84ec34c41c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connor/anaconda3/envs/job_11/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:215: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    }
   ],
   "source": [
    "onnx_model_name= \"location_classifier.onnx\"\n",
    "torch.onnx.export(\n",
    "    loaded_model,\n",
    "    (inputs[\"input_ids\"], inputs[\"attention_mask\"]),\n",
    "    onnx_model_name,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\"}, \n",
    "        \"attention_mask\": {0: \"batch_size\"}, \n",
    "        \"logits\": {0: \"batch_size\"}\n",
    "    },\n",
    "    opset_version=11  # ONNX set version for compatibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8a96f9-327c-4ad1-9184-0bef224e0af2",
   "metadata": {},
   "source": [
    "## Create the .trt model using terminal\n",
    "```\n",
    "trtexec --onnx=location_classifier.onnx --saveEngine=location_classifier.trt \n",
    "```\n",
    "Run the above command without the `--fp16` flag. The 16 precision was making the model not predict the correct entity\n",
    "```\n",
    "trtexec --onnx=location_classifier.onnx --saveEngine=location_classifier_2.trt \\\n",
    "        --minShapes=input_ids:16x256,attention_mask:16x256 \\\n",
    "        --optShapes=input_ids:16x256,attention_mask:16x256 \\\n",
    "        --maxShapes=input_ids:16x256,attention_mask:16x256\n",
    "```\n",
    "To get more memory performance run ^^^ \n",
    "\n",
    "See README for instructions on installing `trtexec`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4bd8eb-d041-4bc3-b682-b23eee34f7a4",
   "metadata": {},
   "source": [
    "# Onnx and Tensorrt Model tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae69b2-35db-436b-bb69-f3f2f32202d8",
   "metadata": {},
   "source": [
    "## Load tokenizer and tokenizer config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd5d8a11-b76f-4c83-9d9d-23fab164e7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connor/anaconda3/envs/job_exp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loaded_tokenizer = BertTokenizer.from_pretrained(tokenizer_out_put_dir)\n",
    "with open(tokenizer_out_put_dir + \"/tokenizer_config.json\", \"r\") as f:\n",
    "    tokenizer_config = json.load(f)\n",
    "\n",
    "max_length = tokenizer_config.get('max_length', 256)\n",
    "truncation = tokenizer_config.get('truncation', True)\n",
    "padding = tokenizer_config.get('padding', 'max_length')\n",
    "is_split_into_words = tokenizer_config.get('is_split_into_words', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22564940-76fe-4a3b-ace9-748648bf4af3",
   "metadata": {},
   "source": [
    "## Test Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ce1b8b-0310-4660-afe8-e33ffe09d36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_location</th>\n",
       "      <th>fmt_raw_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>North Bethesda, MD, Lexington, KY, Remote</td>\n",
       "      <td>north bethesda, md, lexington, ky, remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remote - United Kingdom</td>\n",
       "      <td>remote - united kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remote</td>\n",
       "      <td>remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US - San Francisco</td>\n",
       "      <td>us - san francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remote, USA</td>\n",
       "      <td>remote, usa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                raw_location  \\\n",
       "0  North Bethesda, MD, Lexington, KY, Remote   \n",
       "1                   Remote - United Kingdom    \n",
       "2                                     Remote   \n",
       "3                         US - San Francisco   \n",
       "4                                Remote, USA   \n",
       "\n",
       "                            fmt_raw_location  \n",
       "0  north bethesda, md, lexington, ky, remote  \n",
       "1                    remote - united kingdom  \n",
       "2                                     remote  \n",
       "3                         us - san francisco  \n",
       "4                                remote, usa  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.read_csv('location_data/all_data_fmt_location.csv')\n",
    "all_df.drop(axis=1, inplace=True, columns=['id','title','description'])\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8573b094-92da-4cc0-8cbf-ab94fe44ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = all_df.iloc[1]['fmt_raw_location']\n",
    "inputs = loaded_tokenizer(\n",
    "    text,\n",
    "    max_length=max_length,\n",
    "    truncation=truncation,\n",
    "    padding=padding,\n",
    "    return_tensors=\"np\",\n",
    "    is_split_into_words=is_split_into_words,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0e99af1a-e919-42e7-8558-a1d2b46a1bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remote - united kingdom'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3d3f55ae-ebcf-41b4-8a64-460bfa936b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80e95c-f48d-48fc-98d6-fb67358aa221",
   "metadata": {},
   "source": [
    "## Onnx test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4bfd3774-441f-4344-aab5-b56085ca9abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX predicted class: [[0 1 2 2 2 0 3 0 1 0 3 0 7 0 0 0 0 0 7 0 7 0 1 0 7 0 0 0 0 2 0 3 3 3 3 0\n",
      "  1 0 3 0 5 0 0 0 0 7 0 0 0 1 0 7 0 0 0 0 1 2 2 2 0 3 0 1 0 3 0 0 0 0 0 0\n",
      "  0 0 5 0 3 0 7 0 7 0 0 0 0 1 2 2 2 0 3 0 1 0 3 5 0 0 0 0 0 0 0 7 0 7 0 1\n",
      "  0 7 0 0 2 2 2 0 3 0 1 0 1 0 0 3 0 5 0 0 2 0 1 2 2 2 0 3 0 1 0 7 0 0 0 7\n",
      "  0 0 2 2 3 2 0 3 0 1 1 0 3 0 0 0 7 7 0 0 0 0 0 7 0 0 0 0 0 2 2 1 1 2 2 2\n",
      "  0 3 1 0 1 1 7 2 3 5 5 0 0 0 0 0 0 0 7 7 7 7 1 7 7 0 0 0 7 0 0 0 1 2 2 2\n",
      "  3 0 2 0 5 0 0 0 0 0 2 7 3 3 7 0 1 0 3 0 0 0 0 0 0 0 0 2 3 2 0 3 0 0 0 0\n",
      "  0 0 2 2]]\n"
     ]
    }
   ],
   "source": [
    "# Load ONNX model\n",
    "ort_session = ort.InferenceSession(\"location_classifier.onnx\")\n",
    "\n",
    "# Run inference with ONNX model\n",
    "onnx_outputs = ort_session.run(None, {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_mask\n",
    "})\n",
    "\n",
    "# Logits from the output\n",
    "logits = onnx_outputs[0]  \n",
    "predictions_np = np.argmax(logits, axis=-1)  # Get token-level predictions\n",
    "\n",
    "print(\"ONNX predicted class:\", predictions_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "04a39b00-5e12-45ab-8f7d-d8199f0efde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    1,    2,    2,    2,    0,    3,    0,    1,    0,    3,\n",
       "           0,    7,    0, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use attention mask to only convert the tokens to labels. \n",
    "adjusted_predictions = np.where(attention_mask == 1, predictions_np, -100) \n",
    "adjusted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9dcc009e-d7a2-4ab4-a7fd-68e3e1d7767e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-CITY',\n",
       " 'I-CITY',\n",
       " 'I-CITY',\n",
       " 'I-CITY',\n",
       " 'O',\n",
       " 'B-STATE',\n",
       " 'O',\n",
       " 'B-CITY',\n",
       " 'O',\n",
       " 'B-STATE',\n",
       " 'O',\n",
       " 'B-REMOTE',\n",
       " 'O']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_predictions = adjusted_predictions.flatten() # remove the extra dimension  \n",
    "attention_mask_flat = attention_mask.flatten() # remove extra dimension on np array\n",
    "attention_mask_list = attention_mask_flat.tolist() # convert nparray to list \n",
    "last_real_token_index = len(attention_mask_list) - 1 - attention_mask_list[::-1].index(1) # grab the index of the last real item\n",
    "filtered_predictions_ids = flat_predictions[:last_real_token_index+1] # use last index +1 to remove the -100s\n",
    "predicted_token_labels = [id2label[id] for id in filtered_predictions_ids] # Convert Ids to labels \n",
    "predicted_token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf30ed89-5f07-49b3-9f47-8bd48c218d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'north bethesda, md, lexington, ky, remote'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "138fe9f8-fa5f-4101-aa2f-2dc924b27b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'north',\n",
       " 'beth',\n",
       " '##es',\n",
       " '##da',\n",
       " ',',\n",
       " 'md',\n",
       " ',',\n",
       " 'lexington',\n",
       " ',',\n",
       " 'ky',\n",
       " ',',\n",
       " 'remote',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_list = inputs['input_ids'].squeeze().tolist()  # Remove extra dimension and convert tensor to list  \n",
    "\n",
    "# Grab the tokens for the input example then conver them from ids to actual string token. then Remove the pad tokens. \n",
    "tokens = loaded_tokenizer.convert_ids_to_tokens(input_ids_list) \n",
    "tokens = [token for token in tokens if token != '[PAD]']\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238a30d-7fdc-4616-bac4-c5a6ee7ac4e4",
   "metadata": {},
   "source": [
    "## Tensorrt Usage single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d433dc12-0c57-4761-9bb2-8dbdc8bdf647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorRT engine\n",
    "def load_engine(trt_engine_path):\n",
    "    \"\"\"\n",
    "    Loads the Tensorrt engine\n",
    "    \"\"\"\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "    with open(trt_engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        return runtime.deserialize_cuda_engine(f.read())\n",
    "        \n",
    "def deload_engine(engine, context):\n",
    "    \"\"\"Function to clean up the engine and context.\"\"\"\n",
    "    if context:\n",
    "        context.__del__()  # Explicitly call context destructor\n",
    "    del context  # Delete the context object to free memory\n",
    "    del engine  # Delete the engine object to free memory\n",
    "    \n",
    "def run_inference(context, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Run Input through tensorrt model\n",
    "    \"\"\"\n",
    "    # Allocate memory for inputs and outputs on the GPU\n",
    "    d_input_ids = cuda.mem_alloc(input_ids.nbytes)\n",
    "    d_attention_mask = cuda.mem_alloc(attention_mask.nbytes)\n",
    "\n",
    "    sequence_length = input_ids.shape[1]  # Get the number of tokens in the input\n",
    "    num_labels = 9  \n",
    "    output_shape = (input_ids.shape[0], sequence_length, num_labels)\n",
    "    \n",
    "    d_output = cuda.mem_alloc(int(np.prod(output_shape) * np.dtype(np.float32).itemsize))\n",
    "\n",
    "    # Copy inputs to device memory\n",
    "    cuda.memcpy_htod(d_input_ids, input_ids)\n",
    "    cuda.memcpy_htod(d_attention_mask, attention_mask)\n",
    "\n",
    "    # Run inference\n",
    "    bindings = [int(d_input_ids), int(d_attention_mask), int(d_output)]\n",
    "    context.execute_v2(bindings)\n",
    "\n",
    "    # Copy outputs back to host\n",
    "    output = np.empty(output_shape, dtype=np.float32)\n",
    "    cuda.memcpy_dtoh(output, d_output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "32ef1a36-569e-4617-bba0-a2a7ca13eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = load_engine(\"location_classifier_2.trt\") # Load the TensorRT engine\n",
    "context = engine.create_execution_context()# Create execution context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "75eb5a7a-a969-4044-89fd-a4c7a2d0a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Deload the model \n",
    "# deload_engine(engine, context) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ed12de08-fff3-4426-aa4c-30fe26e07f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds class: [[0 1 2 ... 0 2 2]\n",
      " [0 7 0 ... 7 7 7]\n",
      " [0 7 0 ... 7 7 7]\n",
      " [0 5 0 ... 0 0 0]\n",
      " [0 7 0 ... 1 5 5]]\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "output = run_inference(context, input_ids, attention_mask)\n",
    "predicted_tokens = np.argmax(output, axis=-1)\n",
    "print(f\"Preds class: {predicted_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7581c3e3-2049-4f26-90d4-e87c9bc9b0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "61cac7db-0f8b-4774-925a-2898a121fb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-REMOTE', 'O', 'B-COUNTRY', 'I-COUNTRY', 'O']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_predictions = np.where(attention_mask == 1, predicted_tokens, -100) \n",
    "flat_predictions = adjusted_predictions.flatten() # remove the extra dimension  \n",
    "attention_mask_list = attention_mask.flatten().tolist() # remove extra dimension on np array and convert nparray to list\n",
    "last_real_token_index = len(attention_mask_list) - 1 - attention_mask_list[::-1].index(1) # grab the index of the last real item\n",
    "filtered_predictions_ids = flat_predictions[:last_real_token_index+1] # use last index +1 to remove the -100s\n",
    "predicted_token_labels = [id2label[id] for id in filtered_predictions_ids] # Convert Ids to labels \n",
    "predicted_token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "55864f74-c48d-4ad1-8a69-572af94a2c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'north',\n",
       " 'beth',\n",
       " '##es',\n",
       " '##da',\n",
       " ',',\n",
       " 'md',\n",
       " ',',\n",
       " 'lexington',\n",
       " ',',\n",
       " 'ky',\n",
       " ',',\n",
       " 'remote',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_list = inputs['input_ids'].squeeze().tolist()  # Remove extra dimension and convert tensor to list  \n",
    "# Grab the tokens for the input example then conver them from ids to actual string token. then Remove the pad tokens. \n",
    "tokens = loaded_tokenizer.convert_ids_to_tokens(input_ids_list) \n",
    "tokens = [token for token in tokens if token != '[PAD]']\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b42993a6-4231-4022-aadb-486a070b9193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_token_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "45cdda8c-5347-4acd-a542-ef24aaf876f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "218b82be-89d0-4f90-97d5-0ced671e73e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country_list': [],\n",
       " 'state_list': ['md', 'ky'],\n",
       " 'city_list': ['north bethesda', 'lexington'],\n",
       " 'remote': True}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_final_dict(tokens, predicted_token_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78fb460-50ce-45bc-9001-a3b5e7218a13",
   "metadata": {},
   "source": [
    "## Tensorrt Batch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f8cee4fd-daef-4912-a0b1-76b04e2ed70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_list = all_df['fmt_raw_location'][:5].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c0a7dd6b-a513-4949-9291-ab5bf318729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize batch\n",
    "inputs = loaded_tokenizer(\n",
    "    locations_list,\n",
    "    max_length=max_length,\n",
    "    truncation=truncation,\n",
    "    padding=padding,\n",
    "    return_tensors=\"np\",\n",
    "    is_split_into_words=is_split_into_words,\n",
    ")\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# Run inference on the batch\n",
    "output = run_inference(context, input_ids, attention_mask)\n",
    "\n",
    "# Get predicted classes\n",
    "predicted_tokens = np.argmax(output, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "de614440-e387-44b2-917b-9c3b061a94ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 256)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e234ac-3783-44e4-886f-359c71132a63",
   "metadata": {},
   "source": [
    "### Batch Test Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5ac6fa7c-16bf-487d-a7ac-3ad7776d09cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_final_dict(tokens, pred_labels):\n",
    "    \"\"\"\n",
    "    Takes the tokens and predicted token labels and rebuilds tokens into strings \n",
    "    creates the country, state, city, and remote lists\n",
    "    \"\"\"\n",
    "    entities = {\n",
    "        \"country\": [],\n",
    "        \"state\": [],\n",
    "        \"city\": [],\n",
    "        \"remote\": False \n",
    "    }\n",
    "    \n",
    "    current_entity_tokens = []\n",
    "    current_entity_label = None\n",
    "    \n",
    "    for token, pred_token_label in zip(tokens, pred_labels):\n",
    "        if token in ['[CLS]', '[SEP]']: # Skip special tokens\n",
    "            continue\n",
    "\n",
    "        if pred_token_label == 'B-REMOTE':  # Check for remote first\n",
    "            entities['remote'] = True\n",
    "            continue  \n",
    "        \n",
    "        if pred_token_label != 'O':  # Not an outside word\n",
    "            prefix, entity = pred_token_label.split('-')\n",
    "            if prefix == 'B':  # Start of a new entity\n",
    "                # Save current entity if there is one\n",
    "                if current_entity_label and current_entity_tokens:\n",
    "                    entities[current_entity_label].append(''.join(current_entity_tokens))\n",
    "                \n",
    "                # Start a new entity\n",
    "                current_entity_tokens = [token.replace('##', '')]  # Remove BPE marker for subwords\n",
    "                current_entity_label = entity.lower()  # Start a new entity label\n",
    "            elif prefix == 'I' and entity.lower() == current_entity_label and current_entity_tokens:\n",
    "                # Continue the current entity\n",
    "                if token.startswith('##'):\n",
    "                    current_entity_tokens.append(token.replace('##', ''))\n",
    "                else:\n",
    "                    current_entity_tokens.append(' ' + token)  # Add space if it's a full word\n",
    "        else:\n",
    "            # Save the entity if we hit an 'O' and we have an ongoing entity\n",
    "            if current_entity_label and current_entity_tokens:\n",
    "                entities[current_entity_label].append(''.join(current_entity_tokens))\n",
    "                current_entity_tokens = []\n",
    "                current_entity_label = None\n",
    "    \n",
    "    # Capture any entity that goes right up to the end without hitting an 'O'\n",
    "    if current_entity_label and current_entity_tokens:\n",
    "        entities[current_entity_label].append(''.join(current_entity_tokens))\n",
    "    \n",
    "    return {\n",
    "        \"country_list\": entities['country'], \n",
    "        \"state_list\": entities['state'],\n",
    "        \"city_list\": entities['city'],\n",
    "        \"remote\": entities['remote']\n",
    "    }\n",
    "    \n",
    "state_abbreviation_to_name = {\n",
    "    \"al\": \"alabama\", \"ak\": \"alaska\", \"az\": \"arizona\", \"ar\": \"arkansas\", \"ca\": \"california\", \n",
    "    \"co\": \"colorado\", \"ct\": \"connecticut\", \"de\": \"delaware\", \"fl\": \"florida\", \"ga\": \"georgia\", \n",
    "    \"hi\": \"hawaii\", \"id\": \"idaho\", \"il\": \"illinois\", \"in\": \"indiana\", \"ia\": \"iowa\", \n",
    "    \"ks\": \"kansas\", \"ky\": \"kentucky\", \"la\": \"louisiana\", \"me\": \"maine\", \"md\": \"maryland\", \n",
    "    \"ma\": \"massachusetts\", \"mi\": \"michigan\", \"mn\": \"minnesota\", \"ms\": \"mississippi\", \n",
    "    \"mo\": \"missouri\", \"mt\": \"montana\", \"ne\": \"nebraska\", \"nv\": \"nevada\", \"nh\": \"new hampshire\", \n",
    "    \"nj\": \"new jersey\", \"nm\": \"new mexico\", \"ny\": \"new york\", \"nc\": \"north carolina\", \n",
    "    \"nd\": \"north dakota\", \"oh\": \"ohio\", \"ok\": \"oklahoma\", \"or\": \"oregon\", \"pa\": \"pennsylvania\", \n",
    "    \"ri\": \"rhode island\", \"sc\": \"south carolina\", \"sd\": \"south dakota\", \"tn\": \"tennessee\", \n",
    "    \"tx\": \"texas\", \"ut\": \"utah\", \"vt\": \"vermont\", \"va\": \"virginia\", \"wa\": \"washington\", \n",
    "    \"wv\": \"west virginia\", \"wi\": \"wisconsin\", \"wy\": \"wyoming\"\n",
    "}\n",
    "def normalize_state(state):\n",
    "    if len(state) == 2:\n",
    "        state_name = state_abbreviation_to_name.get(state, None)\n",
    "        if state_name:\n",
    "            return state_name.title()\n",
    "        else:\n",
    "            return None\n",
    "    elif len(state) > 2:\n",
    "        if state in us_state_names:\n",
    "            return state.title()\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "country_abbreviation_to_name = {\n",
    "    \"us\": \"United States\", \"usa\": \"United States\",\n",
    "}\n",
    "def normalize_country(country):\n",
    "    if len(country)<=3:\n",
    "        country_name = country_abbreviation_to_name.get(country, country)\n",
    "        return country_name.title()\n",
    "    else:\n",
    "        return country.title()\n",
    "\n",
    "def normalize_city(city):\n",
    "    return city.title()\n",
    "\n",
    "def process_batch_predictions(predicted_tokens, attention_mask, input_ids, id2label):\n",
    "    batch_predicted_labels = []\n",
    "    token_labels = []\n",
    "    entities =[]\n",
    "\n",
    "    for i in range(predicted_tokens.shape[0]):\n",
    "        adjusted_predictions = np.where(attention_mask[i] == 1, predicted_tokens[i], -100)\n",
    "        flat_predictions = adjusted_predictions.flatten() # Flatten predictions for this example\n",
    "        attention_mask_list = attention_mask[i].flatten().tolist() # Flatten attention mask for this example and convert to list\n",
    "        last_real_token_index = len(attention_mask_list) - 1 - attention_mask_list[::-1].index(1)# Find the index of the last real token (where attention mask is 1)\n",
    "        filtered_predictions_ids = flat_predictions[:last_real_token_index + 1]# Use last index +1 to remove the -100 values for padded tokens\n",
    "        predicted_token_labels = [id2label[id] for id in filtered_predictions_ids]# Convert IDs to their corresponding labels using id2label\n",
    "\n",
    "        input_ids_list = input_ids[i].squeeze().tolist()  # Remove extra dimension and convert tensor to list  \n",
    "        # Grab the tokens for the input example then conver them from ids to actual string token. then Remove the pad tokens. \n",
    "        tokens = loaded_tokenizer.convert_ids_to_tokens(input_ids_list) \n",
    "        token_labels = [token for token in tokens if token != '[PAD]']\n",
    "        \n",
    "        entity_obj = build_final_dict(token_labels, predicted_token_labels)\n",
    "        if len(entity_obj['state_list'])> 0:\n",
    "            new_state_list = []\n",
    "            for state in entity_obj['state_list']:\n",
    "                fmt_state = normalize_state(state)\n",
    "                if fmt_state:\n",
    "                    new_state_list.append(fmt_state)\n",
    "            entity_obj['state_list'] = new_state_list\n",
    "\n",
    "        if len(entity_obj['country_list'])> 0:\n",
    "            for idx in range(len(entity_obj['country_list'])):\n",
    "                entity_obj['country_list'][idx] = normalize_country(entity_obj['country_list'][idx])\n",
    "\n",
    "        if len(entity_obj['city_list'])> 0:\n",
    "            for idx in range(len(entity_obj['city_list'])):\n",
    "                entity_obj['city_list'][idx] = normalize_city(entity_obj['city_list'][idx])\n",
    "                \n",
    "        entities.append(entity_obj)\n",
    "        \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39ec319-48bf-4b7c-b172-d7b12ea1a27d",
   "metadata": {},
   "source": [
    "### Batch Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d829d316-1f34-4222-acfb-04ca4fe4c64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'country_list': [],\n",
       "  'state_list': ['Maryland', 'Kentucky'],\n",
       "  'city_list': ['North Bethesda', 'Lexington'],\n",
       "  'remote': True},\n",
       " {'country_list': ['United Kingdom'],\n",
       "  'state_list': [],\n",
       "  'city_list': [],\n",
       "  'remote': True},\n",
       " {'country_list': [], 'state_list': [], 'city_list': [], 'remote': True},\n",
       " {'country_list': ['United States'],\n",
       "  'state_list': [],\n",
       "  'city_list': ['San Francisco'],\n",
       "  'remote': False},\n",
       " {'country_list': ['United States'],\n",
       "  'state_list': [],\n",
       "  'city_list': [],\n",
       "  'remote': True}]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_batch_predictions(predicted_tokens, attention_mask, input_ids, id2label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
