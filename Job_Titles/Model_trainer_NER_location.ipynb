{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73ea63e-c5d5-4447-958b-3509154499e1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cd999b32-e5ae-4a07-9958-1cd8be9c01e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/connor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DataCollatorWithPadding, DistilBertTokenizer, pipeline, \\\n",
    "                    DistilBertForSequenceClassification, TrainingArguments, Trainer, \\\n",
    "                    DistilBertTokenizerFast, DistilBertForTokenClassification, AutoModelForTokenClassification\n",
    "from transformers import TrainerCallback\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pynvml import *\n",
    "from functools import partial\n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "69e2d799-2522-46c1-a910-e31df6885a93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install nltk\n",
    "# !pip install contractions\n",
    "# !pip install seaborn\n",
    "# !pip install accelerate\n",
    "# !pip install scikit-learn\n",
    "# !pip install evaluate\n",
    "# !pip install pynvml\n",
    "# !pip install seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f09e3-44d2-4522-9d93-187c39dda1f7",
   "metadata": {},
   "source": [
    "# EDA Location Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b446da-0985-4552-8a3e-5b7f9c347a27",
   "metadata": {},
   "source": [
    "## EDA Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ab6c53d9-7917-4d2f-8883-6e77bb20b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    # text = text.replace('(','')\n",
    "    # text = text.replace(')','')\n",
    "    # text = text.replace('-','')\n",
    "    # text = text.replace(',','')\n",
    "    # text = text.replace('–','')\n",
    "    # text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2c11a-075f-4f6c-b6f5-1f75e2d93a85",
   "metadata": {},
   "source": [
    "## Create Data Set For label Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "41ae5523-db2c-4b72-ac7f-0a775b8bc9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>raw_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greenhouse::xometry::4322444007</td>\n",
       "      <td>Vice President, Software Engineering</td>\n",
       "      <td>Xometry (NASDAQ: XMTR) powers the industries o...</td>\n",
       "      <td>North Bethesda, MD, Lexington, KY, Remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>greenhouse::splitmetrics::4285324101</td>\n",
       "      <td>VP of Product</td>\n",
       "      <td>Hi! Its SplitMetrics, a remote-first team of e...</td>\n",
       "      <td>Remote - United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>greenhouse::splitmetrics::4277403101</td>\n",
       "      <td>VP of Product</td>\n",
       "      <td>Hi! Its SplitMetrics, a remote-first team of e...</td>\n",
       "      <td>Remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ashbyhq::airwallex::6096edab-08a6-475a-aa0c-69...</td>\n",
       "      <td>Vice President, Engineering</td>\n",
       "      <td>At Airwallex (airwallex.com), were building th...</td>\n",
       "      <td>US - San Francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lever::centml::8397564a-9cf7-4491-bfd2-b425510...</td>\n",
       "      <td>VP Engineering</td>\n",
       "      <td>About Us We believe AI will fundamentally tran...</td>\n",
       "      <td>Remote, USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>lever::tokenmetrics::5af6e633-25cf-4b1e-826f-1...</td>\n",
       "      <td>Front End Web Developer Intern (Buenos Aires-R...</td>\n",
       "      <td>We are looking for programmers with a keen eye...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>smartrecruiters::EndeavorITSolution::743999656...</td>\n",
       "      <td>Android Internship In Endevor IT Solutions</td>\n",
       "      <td>Company Description Bestowed with high profess...</td>\n",
       "      <td>Indore, MP, in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5320</th>\n",
       "      <td>lever::tokenmetrics::6f57bb88-07ab-4ac4-bc58-7...</td>\n",
       "      <td>Business Analyst Intern (Buenos Aires -Remote)</td>\n",
       "      <td>Token Metrics is searching for a highly capabl...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5321</th>\n",
       "      <td>workday::astrazeneca::wd3::Careers::astrazenec...</td>\n",
       "      <td>Graduate Automation Engineer</td>\n",
       "      <td>Graduate Engineer- AutomationWe have a great o...</td>\n",
       "      <td>Australia – New South Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5322</th>\n",
       "      <td>greenhouse::agoda::5527222</td>\n",
       "      <td>[Tech Cooperative Internship 2024] Software En...</td>\n",
       "      <td>About Agoda  Agoda is an online travel booking...</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5323 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     id  \\\n",
       "0                       greenhouse::xometry::4322444007   \n",
       "1                  greenhouse::splitmetrics::4285324101   \n",
       "2                  greenhouse::splitmetrics::4277403101   \n",
       "3     ashbyhq::airwallex::6096edab-08a6-475a-aa0c-69...   \n",
       "4     lever::centml::8397564a-9cf7-4491-bfd2-b425510...   \n",
       "...                                                 ...   \n",
       "5318  lever::tokenmetrics::5af6e633-25cf-4b1e-826f-1...   \n",
       "5319  smartrecruiters::EndeavorITSolution::743999656...   \n",
       "5320  lever::tokenmetrics::6f57bb88-07ab-4ac4-bc58-7...   \n",
       "5321  workday::astrazeneca::wd3::Careers::astrazenec...   \n",
       "5322                         greenhouse::agoda::5527222   \n",
       "\n",
       "                                                  title  \\\n",
       "0                 Vice President, Software Engineering    \n",
       "1                                         VP of Product   \n",
       "2                                         VP of Product   \n",
       "3                           Vice President, Engineering   \n",
       "4                                        VP Engineering   \n",
       "...                                                 ...   \n",
       "5318  Front End Web Developer Intern (Buenos Aires-R...   \n",
       "5319         Android Internship In Endevor IT Solutions   \n",
       "5320     Business Analyst Intern (Buenos Aires -Remote)   \n",
       "5321                       Graduate Automation Engineer   \n",
       "5322  [Tech Cooperative Internship 2024] Software En...   \n",
       "\n",
       "                                            description  \\\n",
       "0     Xometry (NASDAQ: XMTR) powers the industries o...   \n",
       "1     Hi! Its SplitMetrics, a remote-first team of e...   \n",
       "2     Hi! Its SplitMetrics, a remote-first team of e...   \n",
       "3     At Airwallex (airwallex.com), were building th...   \n",
       "4     About Us We believe AI will fundamentally tran...   \n",
       "...                                                 ...   \n",
       "5318  We are looking for programmers with a keen eye...   \n",
       "5319  Company Description Bestowed with high profess...   \n",
       "5320  Token Metrics is searching for a highly capabl...   \n",
       "5321  Graduate Engineer- AutomationWe have a great o...   \n",
       "5322  About Agoda  Agoda is an online travel booking...   \n",
       "\n",
       "                                   raw_location  \n",
       "0     North Bethesda, MD, Lexington, KY, Remote  \n",
       "1                      Remote - United Kingdom   \n",
       "2                                        Remote  \n",
       "3                            US - San Francisco  \n",
       "4                                   Remote, USA  \n",
       "...                                         ...  \n",
       "5318                                        NaN  \n",
       "5319                             Indore, MP, in  \n",
       "5320                                        NaN  \n",
       "5321                Australia – New South Wales  \n",
       "5322                                  Singapore  \n",
       "\n",
       "[5323 rows x 4 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('all_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8e8c3b5d-20dc-4de2-a788-d98496e05e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.dropna(subset=['raw_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "950f87b9-f662-4b6f-9651-6822047e2172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_120649/3299897354.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['fmt_raw_location'] = new_df['raw_location'].apply(lambda x: preprocess_text(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>raw_location</th>\n",
       "      <th>fmt_raw_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greenhouse::xometry::4322444007</td>\n",
       "      <td>Vice President, Software Engineering</td>\n",
       "      <td>Xometry (NASDAQ: XMTR) powers the industries o...</td>\n",
       "      <td>North Bethesda, MD, Lexington, KY, Remote</td>\n",
       "      <td>north bethesda, md, lexington, ky, remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>greenhouse::splitmetrics::4285324101</td>\n",
       "      <td>VP of Product</td>\n",
       "      <td>Hi! Its SplitMetrics, a remote-first team of e...</td>\n",
       "      <td>Remote - United Kingdom</td>\n",
       "      <td>remote - united kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>greenhouse::splitmetrics::4277403101</td>\n",
       "      <td>VP of Product</td>\n",
       "      <td>Hi! Its SplitMetrics, a remote-first team of e...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ashbyhq::airwallex::6096edab-08a6-475a-aa0c-69...</td>\n",
       "      <td>Vice President, Engineering</td>\n",
       "      <td>At Airwallex (airwallex.com), were building th...</td>\n",
       "      <td>US - San Francisco</td>\n",
       "      <td>us - san francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lever::centml::8397564a-9cf7-4491-bfd2-b425510...</td>\n",
       "      <td>VP Engineering</td>\n",
       "      <td>About Us We believe AI will fundamentally tran...</td>\n",
       "      <td>Remote, USA</td>\n",
       "      <td>remote, usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>lever::tokenmetrics::f28bdf66-e2f6-48f6-a5fa-6...</td>\n",
       "      <td>Crypto QA Automation Engineer Intern (Sao Paul...</td>\n",
       "      <td>Token Metrics is looking for an experienced QA...</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>são paulo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>lever::tokenmetrics::4c479c69-0fae-4489-bd74-e...</td>\n",
       "      <td>Crypto Engineering Manager Intern (Bucharest-R...</td>\n",
       "      <td>We are looking for a trustworthy and proactive...</td>\n",
       "      <td>Bucharest</td>\n",
       "      <td>bucharest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>smartrecruiters::EndeavorITSolution::743999656...</td>\n",
       "      <td>Android Internship In Endevor IT Solutions</td>\n",
       "      <td>Company Description Bestowed with high profess...</td>\n",
       "      <td>Indore, MP, in</td>\n",
       "      <td>indore, mp, in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5321</th>\n",
       "      <td>workday::astrazeneca::wd3::Careers::astrazenec...</td>\n",
       "      <td>Graduate Automation Engineer</td>\n",
       "      <td>Graduate Engineer- AutomationWe have a great o...</td>\n",
       "      <td>Australia – New South Wales</td>\n",
       "      <td>australia – new south wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5322</th>\n",
       "      <td>greenhouse::agoda::5527222</td>\n",
       "      <td>[Tech Cooperative Internship 2024] Software En...</td>\n",
       "      <td>About Agoda  Agoda is an online travel booking...</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>singapore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5218 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     id  \\\n",
       "0                       greenhouse::xometry::4322444007   \n",
       "1                  greenhouse::splitmetrics::4285324101   \n",
       "2                  greenhouse::splitmetrics::4277403101   \n",
       "3     ashbyhq::airwallex::6096edab-08a6-475a-aa0c-69...   \n",
       "4     lever::centml::8397564a-9cf7-4491-bfd2-b425510...   \n",
       "...                                                 ...   \n",
       "5316  lever::tokenmetrics::f28bdf66-e2f6-48f6-a5fa-6...   \n",
       "5317  lever::tokenmetrics::4c479c69-0fae-4489-bd74-e...   \n",
       "5319  smartrecruiters::EndeavorITSolution::743999656...   \n",
       "5321  workday::astrazeneca::wd3::Careers::astrazenec...   \n",
       "5322                         greenhouse::agoda::5527222   \n",
       "\n",
       "                                                  title  \\\n",
       "0                 Vice President, Software Engineering    \n",
       "1                                         VP of Product   \n",
       "2                                         VP of Product   \n",
       "3                           Vice President, Engineering   \n",
       "4                                        VP Engineering   \n",
       "...                                                 ...   \n",
       "5316  Crypto QA Automation Engineer Intern (Sao Paul...   \n",
       "5317  Crypto Engineering Manager Intern (Bucharest-R...   \n",
       "5319         Android Internship In Endevor IT Solutions   \n",
       "5321                       Graduate Automation Engineer   \n",
       "5322  [Tech Cooperative Internship 2024] Software En...   \n",
       "\n",
       "                                            description  \\\n",
       "0     Xometry (NASDAQ: XMTR) powers the industries o...   \n",
       "1     Hi! Its SplitMetrics, a remote-first team of e...   \n",
       "2     Hi! Its SplitMetrics, a remote-first team of e...   \n",
       "3     At Airwallex (airwallex.com), were building th...   \n",
       "4     About Us We believe AI will fundamentally tran...   \n",
       "...                                                 ...   \n",
       "5316  Token Metrics is looking for an experienced QA...   \n",
       "5317  We are looking for a trustworthy and proactive...   \n",
       "5319  Company Description Bestowed with high profess...   \n",
       "5321  Graduate Engineer- AutomationWe have a great o...   \n",
       "5322  About Agoda  Agoda is an online travel booking...   \n",
       "\n",
       "                                   raw_location  \\\n",
       "0     North Bethesda, MD, Lexington, KY, Remote   \n",
       "1                      Remote - United Kingdom    \n",
       "2                                        Remote   \n",
       "3                            US - San Francisco   \n",
       "4                                   Remote, USA   \n",
       "...                                         ...   \n",
       "5316                                  São Paulo   \n",
       "5317                                  Bucharest   \n",
       "5319                             Indore, MP, in   \n",
       "5321                Australia – New South Wales   \n",
       "5322                                  Singapore   \n",
       "\n",
       "                               fmt_raw_location  \n",
       "0     north bethesda, md, lexington, ky, remote  \n",
       "1                       remote - united kingdom  \n",
       "2                                        remote  \n",
       "3                            us - san francisco  \n",
       "4                                   remote, usa  \n",
       "...                                         ...  \n",
       "5316                                  são paulo  \n",
       "5317                                  bucharest  \n",
       "5319                             indore, mp, in  \n",
       "5321                australia – new south wales  \n",
       "5322                                  singapore  \n",
       "\n",
       "[5218 rows x 5 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['fmt_raw_location'] = new_df['raw_location'].apply(lambda x: preprocess_text(x))\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5c7dc3e3-bf0f-43b6-8283-bfd7d594fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('all_data_fmt_location.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "72efa55a-9d79-4208-bfca-fd73682f0cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>raw_location</th>\n",
       "      <th>fmt_raw_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greenhouse::xometry::4322444007</td>\n",
       "      <td>Vice President, Software Engineering</td>\n",
       "      <td>Xometry (NASDAQ: XMTR) powers the industries o...</td>\n",
       "      <td>North Bethesda, MD, Lexington, KY, Remote</td>\n",
       "      <td>north bethesda, md, lexington, ky, remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>greenhouse::splitmetrics::4285324101</td>\n",
       "      <td>VP of Product</td>\n",
       "      <td>Hi! Its SplitMetrics, a remote-first team of e...</td>\n",
       "      <td>Remote - United Kingdom</td>\n",
       "      <td>remote - united kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>greenhouse::splitmetrics::4277403101</td>\n",
       "      <td>VP of Product</td>\n",
       "      <td>Hi! Its SplitMetrics, a remote-first team of e...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ashbyhq::airwallex::6096edab-08a6-475a-aa0c-69...</td>\n",
       "      <td>Vice President, Engineering</td>\n",
       "      <td>At Airwallex (airwallex.com), were building th...</td>\n",
       "      <td>US - San Francisco</td>\n",
       "      <td>us - san francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lever::centml::8397564a-9cf7-4491-bfd2-b425510...</td>\n",
       "      <td>VP Engineering</td>\n",
       "      <td>About Us We believe AI will fundamentally tran...</td>\n",
       "      <td>Remote, USA</td>\n",
       "      <td>remote, usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5213</th>\n",
       "      <td>lever::tokenmetrics::f28bdf66-e2f6-48f6-a5fa-6...</td>\n",
       "      <td>Crypto QA Automation Engineer Intern (Sao Paul...</td>\n",
       "      <td>Token Metrics is looking for an experienced QA...</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>são paulo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214</th>\n",
       "      <td>lever::tokenmetrics::4c479c69-0fae-4489-bd74-e...</td>\n",
       "      <td>Crypto Engineering Manager Intern (Bucharest-R...</td>\n",
       "      <td>We are looking for a trustworthy and proactive...</td>\n",
       "      <td>Bucharest</td>\n",
       "      <td>bucharest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>smartrecruiters::EndeavorITSolution::743999656...</td>\n",
       "      <td>Android Internship In Endevor IT Solutions</td>\n",
       "      <td>Company Description Bestowed with high profess...</td>\n",
       "      <td>Indore, MP, in</td>\n",
       "      <td>indore, mp, in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>workday::astrazeneca::wd3::Careers::astrazenec...</td>\n",
       "      <td>Graduate Automation Engineer</td>\n",
       "      <td>Graduate Engineer- AutomationWe have a great o...</td>\n",
       "      <td>Australia – New South Wales</td>\n",
       "      <td>australia – new south wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>greenhouse::agoda::5527222</td>\n",
       "      <td>[Tech Cooperative Internship 2024] Software En...</td>\n",
       "      <td>About Agoda  Agoda is an online travel booking...</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>singapore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5218 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     id  \\\n",
       "0                       greenhouse::xometry::4322444007   \n",
       "1                  greenhouse::splitmetrics::4285324101   \n",
       "2                  greenhouse::splitmetrics::4277403101   \n",
       "3     ashbyhq::airwallex::6096edab-08a6-475a-aa0c-69...   \n",
       "4     lever::centml::8397564a-9cf7-4491-bfd2-b425510...   \n",
       "...                                                 ...   \n",
       "5213  lever::tokenmetrics::f28bdf66-e2f6-48f6-a5fa-6...   \n",
       "5214  lever::tokenmetrics::4c479c69-0fae-4489-bd74-e...   \n",
       "5215  smartrecruiters::EndeavorITSolution::743999656...   \n",
       "5216  workday::astrazeneca::wd3::Careers::astrazenec...   \n",
       "5217                         greenhouse::agoda::5527222   \n",
       "\n",
       "                                                  title  \\\n",
       "0                 Vice President, Software Engineering    \n",
       "1                                         VP of Product   \n",
       "2                                         VP of Product   \n",
       "3                           Vice President, Engineering   \n",
       "4                                        VP Engineering   \n",
       "...                                                 ...   \n",
       "5213  Crypto QA Automation Engineer Intern (Sao Paul...   \n",
       "5214  Crypto Engineering Manager Intern (Bucharest-R...   \n",
       "5215         Android Internship In Endevor IT Solutions   \n",
       "5216                       Graduate Automation Engineer   \n",
       "5217  [Tech Cooperative Internship 2024] Software En...   \n",
       "\n",
       "                                            description  \\\n",
       "0     Xometry (NASDAQ: XMTR) powers the industries o...   \n",
       "1     Hi! Its SplitMetrics, a remote-first team of e...   \n",
       "2     Hi! Its SplitMetrics, a remote-first team of e...   \n",
       "3     At Airwallex (airwallex.com), were building th...   \n",
       "4     About Us We believe AI will fundamentally tran...   \n",
       "...                                                 ...   \n",
       "5213  Token Metrics is looking for an experienced QA...   \n",
       "5214  We are looking for a trustworthy and proactive...   \n",
       "5215  Company Description Bestowed with high profess...   \n",
       "5216  Graduate Engineer- AutomationWe have a great o...   \n",
       "5217  About Agoda  Agoda is an online travel booking...   \n",
       "\n",
       "                                   raw_location  \\\n",
       "0     North Bethesda, MD, Lexington, KY, Remote   \n",
       "1                      Remote - United Kingdom    \n",
       "2                                        Remote   \n",
       "3                            US - San Francisco   \n",
       "4                                   Remote, USA   \n",
       "...                                         ...   \n",
       "5213                                  São Paulo   \n",
       "5214                                  Bucharest   \n",
       "5215                             Indore, MP, in   \n",
       "5216                Australia – New South Wales   \n",
       "5217                                  Singapore   \n",
       "\n",
       "                               fmt_raw_location  \n",
       "0     north bethesda, md, lexington, ky, remote  \n",
       "1                       remote - united kingdom  \n",
       "2                                        remote  \n",
       "3                            us - san francisco  \n",
       "4                                   remote, usa  \n",
       "...                                         ...  \n",
       "5213                                  são paulo  \n",
       "5214                                  bucharest  \n",
       "5215                             indore, mp, in  \n",
       "5216                australia – new south wales  \n",
       "5217                                  singapore  \n",
       "\n",
       "[5218 rows x 5 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('all_data_fmt_location.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c589fc48-67fd-4259-8cdd-1c6a8468858a",
   "metadata": {},
   "source": [
    "# Model Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1158c24-25b7-4805-981a-a51b7ea270b5",
   "metadata": {},
   "source": [
    "## Load Label Studio Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7a4be510-eba1-4354-83e6-0015e2c20509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>raw_location</th>\n",
       "      <th>fmt_raw_location</th>\n",
       "      <th>label</th>\n",
       "      <th>annotator</th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>lead_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4155</td>\n",
       "      <td>Vice President, Software Engineering</td>\n",
       "      <td>Xometry (NASDAQ: XMTR) powers the industries o...</td>\n",
       "      <td>North Bethesda, MD, Lexington, KY, Remote</td>\n",
       "      <td>north bethesda, md, lexington, ky, remote</td>\n",
       "      <td>[{'start': 16, 'end': 18, 'text': 'md', 'label...</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>2024-04-09 21:34:21.146053+00:00</td>\n",
       "      <td>2024-04-09 21:34:21.146086+00:00</td>\n",
       "      <td>14.943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4156</td>\n",
       "      <td>VP of Product</td>\n",
       "      <td>Hi! Its SplitMetrics, a remote-first team of e...</td>\n",
       "      <td>Remote - United Kingdom</td>\n",
       "      <td>remote - united kingdom</td>\n",
       "      <td>[{'start': 9, 'end': 23, 'text': 'united kingd...</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>2024-04-09 21:34:34.088538+00:00</td>\n",
       "      <td>2024-04-09 21:34:34.088571+00:00</td>\n",
       "      <td>10.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4157</td>\n",
       "      <td>VP of Product</td>\n",
       "      <td>Hi! Its SplitMetrics, a remote-first team of e...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>remote</td>\n",
       "      <td>[{'start': 0, 'end': 6, 'text': 'remote', 'lab...</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>2024-04-09 21:34:39.463909+00:00</td>\n",
       "      <td>2024-04-09 21:34:39.463942+00:00</td>\n",
       "      <td>3.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4158</td>\n",
       "      <td>Vice President, Engineering</td>\n",
       "      <td>At Airwallex (airwallex.com), were building th...</td>\n",
       "      <td>US - San Francisco</td>\n",
       "      <td>us - san francisco</td>\n",
       "      <td>[{'start': 5, 'end': 18, 'text': 'san francisc...</td>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "      <td>2024-04-09 21:34:49.208866+00:00</td>\n",
       "      <td>2024-04-09 21:34:49.208903+00:00</td>\n",
       "      <td>6.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4159</td>\n",
       "      <td>VP Engineering</td>\n",
       "      <td>About Us We believe AI will fundamentally tran...</td>\n",
       "      <td>Remote, USA</td>\n",
       "      <td>remote, usa</td>\n",
       "      <td>[{'start': 8, 'end': 11, 'text': 'usa', 'label...</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>2024-04-09 21:34:58.376343+00:00</td>\n",
       "      <td>2024-04-09 21:34:58.376374+00:00</td>\n",
       "      <td>6.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>4319</td>\n",
       "      <td>VP Product, Hardware</td>\n",
       "      <td>Location: Berlin (On-site) At SumUp, we are mo...</td>\n",
       "      <td>Berlin, Germany</td>\n",
       "      <td>berlin, germany</td>\n",
       "      <td>[{'start': 0, 'end': 6, 'text': 'berlin', 'lab...</td>\n",
       "      <td>2</td>\n",
       "      <td>162</td>\n",
       "      <td>2024-04-09 22:04:30.532149+00:00</td>\n",
       "      <td>2024-04-09 22:04:30.532181+00:00</td>\n",
       "      <td>4.245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>4322</td>\n",
       "      <td>VP, Product Management - IT Leaders</td>\n",
       "      <td>This is a unique opportunity to make an impact...</td>\n",
       "      <td>2 Locations</td>\n",
       "      <td>2 locations</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>2024-04-09 22:04:35.792296+00:00</td>\n",
       "      <td>2024-04-09 22:04:35.792331+00:00</td>\n",
       "      <td>2.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>4328</td>\n",
       "      <td>Vice President, GSI and Consulting Partnership</td>\n",
       "      <td>Sonar solves the trillion-dollar challenge of ...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>austin</td>\n",
       "      <td>[{'start': 0, 'end': 6, 'text': 'austin', 'lab...</td>\n",
       "      <td>2</td>\n",
       "      <td>175</td>\n",
       "      <td>2024-04-09 22:14:11.679114+00:00</td>\n",
       "      <td>2024-04-09 22:14:11.679147+00:00</td>\n",
       "      <td>2.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>4329</td>\n",
       "      <td>VP Engineering</td>\n",
       "      <td>About Safe:Safe is the account abstraction lea...</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>berlin</td>\n",
       "      <td>[{'start': 0, 'end': 6, 'text': 'berlin', 'lab...</td>\n",
       "      <td>2</td>\n",
       "      <td>176</td>\n",
       "      <td>2024-04-09 22:14:15.587134+00:00</td>\n",
       "      <td>2024-04-09 22:14:15.587156+00:00</td>\n",
       "      <td>2.196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>4331</td>\n",
       "      <td>Software Engineer - Client Tech Vice President</td>\n",
       "      <td>AQR Capital Management AQR is a global investm...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>bengaluru</td>\n",
       "      <td>[{'start': 0, 'end': 9, 'text': 'bengaluru', '...</td>\n",
       "      <td>2</td>\n",
       "      <td>177</td>\n",
       "      <td>2024-04-09 22:14:25.949009+00:00</td>\n",
       "      <td>2024-04-09 22:14:25.949028+00:00</td>\n",
       "      <td>8.587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                           title  \\\n",
       "0    4155           Vice President, Software Engineering    \n",
       "1    4156                                   VP of Product   \n",
       "2    4157                                   VP of Product   \n",
       "3    4158                     Vice President, Engineering   \n",
       "4    4159                                  VP Engineering   \n",
       "..    ...                                             ...   \n",
       "111  4319                            VP Product, Hardware   \n",
       "112  4322             VP, Product Management - IT Leaders   \n",
       "113  4328  Vice President, GSI and Consulting Partnership   \n",
       "114  4329                                  VP Engineering   \n",
       "115  4331  Software Engineer - Client Tech Vice President   \n",
       "\n",
       "                                           description  \\\n",
       "0    Xometry (NASDAQ: XMTR) powers the industries o...   \n",
       "1    Hi! Its SplitMetrics, a remote-first team of e...   \n",
       "2    Hi! Its SplitMetrics, a remote-first team of e...   \n",
       "3    At Airwallex (airwallex.com), were building th...   \n",
       "4    About Us We believe AI will fundamentally tran...   \n",
       "..                                                 ...   \n",
       "111  Location: Berlin (On-site) At SumUp, we are mo...   \n",
       "112  This is a unique opportunity to make an impact...   \n",
       "113  Sonar solves the trillion-dollar challenge of ...   \n",
       "114  About Safe:Safe is the account abstraction lea...   \n",
       "115  AQR Capital Management AQR is a global investm...   \n",
       "\n",
       "                                  raw_location  \\\n",
       "0    North Bethesda, MD, Lexington, KY, Remote   \n",
       "1                     Remote - United Kingdom    \n",
       "2                                       Remote   \n",
       "3                           US - San Francisco   \n",
       "4                                  Remote, USA   \n",
       "..                                         ...   \n",
       "111                            Berlin, Germany   \n",
       "112                                2 Locations   \n",
       "113                                     Austin   \n",
       "114                                     Berlin   \n",
       "115                                  Bengaluru   \n",
       "\n",
       "                              fmt_raw_location  \\\n",
       "0    north bethesda, md, lexington, ky, remote   \n",
       "1                      remote - united kingdom   \n",
       "2                                       remote   \n",
       "3                           us - san francisco   \n",
       "4                                  remote, usa   \n",
       "..                                         ...   \n",
       "111                            berlin, germany   \n",
       "112                                2 locations   \n",
       "113                                     austin   \n",
       "114                                     berlin   \n",
       "115                                  bengaluru   \n",
       "\n",
       "                                                 label  annotator  \\\n",
       "0    [{'start': 16, 'end': 18, 'text': 'md', 'label...          2   \n",
       "1    [{'start': 9, 'end': 23, 'text': 'united kingd...          2   \n",
       "2    [{'start': 0, 'end': 6, 'text': 'remote', 'lab...          2   \n",
       "3    [{'start': 5, 'end': 18, 'text': 'san francisc...          2   \n",
       "4    [{'start': 8, 'end': 11, 'text': 'usa', 'label...          2   \n",
       "..                                                 ...        ...   \n",
       "111  [{'start': 0, 'end': 6, 'text': 'berlin', 'lab...          2   \n",
       "112                                                 []          2   \n",
       "113  [{'start': 0, 'end': 6, 'text': 'austin', 'lab...          2   \n",
       "114  [{'start': 0, 'end': 6, 'text': 'berlin', 'lab...          2   \n",
       "115  [{'start': 0, 'end': 9, 'text': 'bengaluru', '...          2   \n",
       "\n",
       "     annotation_id                       created_at  \\\n",
       "0               62 2024-04-09 21:34:21.146053+00:00   \n",
       "1               63 2024-04-09 21:34:34.088538+00:00   \n",
       "2               64 2024-04-09 21:34:39.463909+00:00   \n",
       "3               65 2024-04-09 21:34:49.208866+00:00   \n",
       "4               66 2024-04-09 21:34:58.376343+00:00   \n",
       "..             ...                              ...   \n",
       "111            162 2024-04-09 22:04:30.532149+00:00   \n",
       "112            163 2024-04-09 22:04:35.792296+00:00   \n",
       "113            175 2024-04-09 22:14:11.679114+00:00   \n",
       "114            176 2024-04-09 22:14:15.587134+00:00   \n",
       "115            177 2024-04-09 22:14:25.949009+00:00   \n",
       "\n",
       "                          updated_at  lead_time  \n",
       "0   2024-04-09 21:34:21.146086+00:00     14.943  \n",
       "1   2024-04-09 21:34:34.088571+00:00     10.999  \n",
       "2   2024-04-09 21:34:39.463942+00:00      3.831  \n",
       "3   2024-04-09 21:34:49.208903+00:00      6.598  \n",
       "4   2024-04-09 21:34:58.376374+00:00      6.864  \n",
       "..                               ...        ...  \n",
       "111 2024-04-09 22:04:30.532181+00:00      4.245  \n",
       "112 2024-04-09 22:04:35.792331+00:00      2.514  \n",
       "113 2024-04-09 22:14:11.679147+00:00      2.546  \n",
       "114 2024-04-09 22:14:15.587156+00:00      2.196  \n",
       "115 2024-04-09 22:14:25.949028+00:00      8.587  \n",
       "\n",
       "[116 rows x 11 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('label-studio-export.json')\n",
    "df['label'] = df['label'].apply(lambda x: [] if x is np.nan else x)# Replace nans with an empty list \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5cb2217a-8a9c-4a4e-a469-9c0b1419b111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 16, 'end': 18, 'text': 'md', 'labels': ['state']},\n",
       " {'start': 31, 'end': 33, 'text': 'ky', 'labels': ['state']},\n",
       " {'start': 35, 'end': 41, 'text': 'remote', 'labels': ['remote']},\n",
       " {'start': 0, 'end': 14, 'text': 'north bethesda', 'labels': ['city']},\n",
       " {'start': 20, 'end': 29, 'text': 'lexington', 'labels': ['city']}]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a55e9d-9066-4f84-93ad-221976512707",
   "metadata": {},
   "source": [
    "## NER Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d1836779-e5c3-4c9e-9390-ddb005770715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ner_label_mappings(token_labels):\n",
    "    label2id = {'O': 0}\n",
    "    id2label = {0: 'O'}\n",
    "    index = 1  \n",
    "    for label in token_labels:\n",
    "        for prefix in ['B-', 'I-']:\n",
    "            current_label = f\"{prefix}{label.upper()}\"\n",
    "            label2id[current_label] = index\n",
    "            id2label[index] = current_label\n",
    "            index += 1\n",
    "    print(f\"Num Labels: {len(label2id.keys())}\")\n",
    "    print(f\"label2id: {label2id}\")\n",
    "    print(f\"id2label: {id2label}\")\n",
    "    return label2id, id2label\n",
    "\n",
    "def format_rows_for_ner_train(df):\n",
    "    final_df = df[['fmt_raw_location','label']].copy()\n",
    "    final_df.rename(columns = {'fmt_raw_location':'text'}, inplace = True) \n",
    "    \n",
    "    training_rows_DS = Dataset.from_pandas(final_df)\n",
    "    training_rows_DS = training_rows_DS.train_test_split(train_size=.8, seed=42) \n",
    "\n",
    "    train_rows = pd.DataFrame(list(zip(training_rows_DS['train']['text'], training_rows_DS['train']['label'])), columns =['text','label'])\n",
    "    test_rows = pd.DataFrame(list(zip(training_rows_DS['test']['text'], training_rows_DS['test']['label'])), columns =['text','label'])\n",
    "    \n",
    "    return training_rows_DS, train_rows, test_rows\n",
    "\n",
    "def align_label(tokenized_inputs, label_list):\n",
    "    if isinstance(tokenized_inputs['input_ids'], torch.Tensor):\n",
    "        label_length = tokenized_inputs['input_ids'].size()[1]\n",
    "    else:\n",
    "        label_length = len(tokenized_inputs['input_ids'])\n",
    "    \n",
    "    example_labels = ['O'] * label_length # Initialize all tokens as 'O'\n",
    "\n",
    "    if isinstance(tokenized_inputs['offset_mapping'], torch.Tensor):\n",
    "        offset_mapping_list = tokenized_inputs['offset_mapping'][0] \n",
    "        offset_mapping_len  = offset_mapping_list.size()[0]  \n",
    "    else:\n",
    "        offset_mapping_list = tokenized_inputs['offset_mapping']\n",
    "        offset_mapping_len  = len(offset_mapping_list)\n",
    "        \n",
    "    for label in label_list:\n",
    "        label_start = label['start']\n",
    "        label_end = label['end']\n",
    "        label_type = label['labels'][0]\n",
    "\n",
    "        # Assign B- and I- labels\n",
    "        for idx, (start, end) in enumerate(offset_mapping_list):\n",
    "            if idx == 0 or idx == offset_mapping_len - 1 or start == end:\n",
    "                continue\n",
    "            if start >= label_start and end <= label_end:\n",
    "                if start == label_start:\n",
    "                    example_labels[idx] = f\"B-{label_type.upper()}\"\n",
    "                else:\n",
    "                    example_labels[idx] = f\"I-{label_type.upper()}\"\n",
    "\n",
    "    return example_labels\n",
    "\n",
    "def align_labels_with_tokens_batched(batch, tokenizer, label2id, tokenizer_config, **kwargs):\n",
    "    batch_input_ids, batch_attention_mask, batch_labels = [], [], []\n",
    "\n",
    "    for i in range(len(batch['text'])):\n",
    "        tokenized_inputs = tokenizer(batch['text'][i], **tokenizer_config)\n",
    "        example_labels = align_label(tokenized_inputs, label_list = batch['label'][i])\n",
    "\n",
    "        # Add tokenized inputs and labels for this example to the batch\n",
    "        batch_input_ids.append(tokenized_inputs['input_ids'])\n",
    "        batch_attention_mask.append(tokenized_inputs['attention_mask'])\n",
    "\n",
    "        # label_ids = [label2id[label] for label in example_labels]# Old way where cls and sep were labeled as 0. New way is below. \n",
    "        \n",
    "        # Convert labels to numerical IDs; label special tokens and padding as -100\n",
    "        last_real_token_index = len(tokenized_inputs['attention_mask']) - 1 - tokenized_inputs['attention_mask'][::-1].index(1)    \n",
    "        label_ids = [\n",
    "            -100 if i == 0 or i == last_real_token_index or tokenized_inputs['attention_mask'][i] == 0 else label2id.get(label, label2id['O'])\n",
    "            for i, label in enumerate(example_labels)\n",
    "        ]\n",
    "        \n",
    "        batch_labels.append(label_ids)  # Convert labels to numerical IDs\n",
    "        \n",
    "        # Remove offset_mapping  \n",
    "        tokenized_inputs.pop('offset_mapping', None)\n",
    "\n",
    "    return {\n",
    "        'input_ids': batch_input_ids,\n",
    "        'attention_mask': batch_attention_mask,\n",
    "        'labels': batch_labels\n",
    "    }\n",
    "    \n",
    "def align_labels_with_tokens_single(example, tokenizer, label2id, tokenizer_config, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to align labels with tokens for a single example. Can handle returning pts from tokenizer or standard lists. \n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(example['text'], **tokenizer_config)\n",
    "    example_labels = align_label(tokenized_inputs, label_list = example['label'])\n",
    "    # Remove offsets mapping \n",
    "    tokenized_inputs.pop(\"offset_mapping\", None)\n",
    "    # tokenized_inputs['labels'] = [label2id[label] for label in example_labels] # Old way where cls and sep were labeled as 0. New way is below. \n",
    "\n",
    "\n",
    "    # Convert labels to numerical IDs; label special tokens and padding as -100\n",
    "    if isinstance(tokenized_inputs['attention_mask'], torch.Tensor):\n",
    "        attention_mask_list = tokenized_inputs['attention_mask'][0].tolist()            \n",
    "    else:\n",
    "        attention_mask_list = tokenized_inputs['attention_mask']\n",
    "        \n",
    "    last_real_token_index = len(attention_mask_list) - 1 - attention_mask_list[::-1].index(1)\n",
    "    tokenized_inputs['labels'] = [\n",
    "        -100 if i == 0 or i == last_real_token_index or attention_mask_list[i] == 0 else label2id.get(label, label2id['O'])\n",
    "        for i, label in enumerate(example_labels)\n",
    "    ]\n",
    "    return tokenized_inputs\n",
    "\n",
    "def custom_data_collator(features):\n",
    "    \"\"\"\n",
    "    Custom data collator that bypasses tokenization and padding.\n",
    "    Assumes that all preprocessing, including tokenization and padding, has been handled.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        for key, value in feature.items():\n",
    "            if isinstance(value, list):\n",
    "                feature[key] = torch.tensor(value)\n",
    "    return default_collate(features)\n",
    "\n",
    "def compute_metrics_token_classification(pred):\n",
    "    predictions = np.argmax(pred.predictions, axis=2)\n",
    "    true_labels = pred.label_ids\n",
    "\n",
    "    pred_labels = [[id2label[p] for (p, l) in zip(prediction, true_label) if l != -100] for prediction, true_label in zip(predictions, true_labels)]\n",
    "    true_labels = [[id2label[l] for (p, l) in zip(prediction, true_label) if l != -100] for prediction, true_label in zip(predictions, true_labels)]\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, pred_labels),\n",
    "        \"recall\": recall_score(true_labels, pred_labels),\n",
    "        \"f1\": f1_score(true_labels, pred_labels),\n",
    "        \"report\": classification_report(true_labels, pred_labels),\n",
    "        \"pred_labels\": pred_labels,\n",
    "        \"true_labels\": true_labels\n",
    "    }\n",
    "\n",
    "class selective_logging_callback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # Filter out the  report and labels from logs before printing\n",
    "        if logs:\n",
    "            logs.pop(\"report\", None)  # Remove report from logs\n",
    "            logs.pop(\"pred_labels\", None) \n",
    "            logs.pop(\"true_labels\", None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882731a5-517f-4a06-928f-2cb7b06c7497",
   "metadata": {},
   "source": [
    "## Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d6fef24a-68a2-44d4-9ca5-3085e35d6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_v10(train_rows, test_rows, preprocess_funct_batched, tokenizer, tokenizer_config, num_epochs, train_batch_size, test_batch_size, base_model, out_put_dir, compute_metrics,  should_return=False):\n",
    "    \n",
    "    train_rows_DS = Dataset.from_pandas(train_rows)\n",
    "    test_rows_DS = Dataset.from_pandas(test_rows)\n",
    "        \n",
    "    train_rows_DS = train_rows_DS.map(preprocess_funct_batched, batched=True, remove_columns=[\"text\", \"label\"])\n",
    "    test_rows_DS = test_rows_DS.map(preprocess_funct_batched, batched=True, remove_columns=[\"text\", \"label\"])\n",
    "\n",
    "    # base_model = accelerator.prepare(base_model)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_put_dir,\n",
    "        overwrite_output_dir=True, \n",
    "        num_train_epochs=num_epochs, \n",
    "        per_device_train_batch_size=train_batch_size, \n",
    "        per_device_eval_batch_size=test_batch_size,  \n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps=5,\n",
    "        log_level='info',\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch'\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=base_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_rows_DS,\n",
    "        eval_dataset=test_rows_DS,\n",
    "        data_collator=custom_data_collator,\n",
    "        compute_metrics=compute_metrics_token_classification,\n",
    "        callbacks=[selective_logging_callback()]\n",
    "    )\n",
    "    \n",
    "    eval_results_before = trainer.evaluate()\n",
    "    \n",
    "    train_results = trainer.train()\n",
    "    \n",
    "    eval_results_after = trainer.evaluate()\n",
    "    \n",
    "    trainer.save_model()\n",
    "    return eval_results_after\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e03fff-b61c-4134-9224-033a2201e930",
   "metadata": {},
   "source": [
    "## Setup Data for model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cf0555cd-076e-4e9d-8021-97628be8049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rows_DS, train_rows, test_rows = format_rows_for_ner_train(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371aef7e-d865-48c1-9565-2c2db33c5f01",
   "metadata": {},
   "source": [
    "## Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1199c87a-4776-4840-8a86-1463bc037123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Labels: 9\n",
      "label2id: {'O': 0, 'B-CITY': 1, 'I-CITY': 2, 'B-STATE': 3, 'I-STATE': 4, 'B-COUNTRY': 5, 'I-COUNTRY': 6, 'B-REMOTE': 7, 'I-REMOTE': 8}\n",
      "id2label: {0: 'O', 1: 'B-CITY', 2: 'I-CITY', 3: 'B-STATE', 4: 'I-STATE', 5: 'B-COUNTRY', 6: 'I-COUNTRY', 7: 'B-REMOTE', 8: 'I-REMOTE'}\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "max_length=256\n",
    "out_put_dir = f\"./location_bert\"\n",
    "tokenizer_config = {'max_length': max_length,\n",
    "                    'truncation': True,\n",
    "                    'padding': 'max_length',\n",
    "                    'return_offsets_mapping':True,\n",
    "                    'is_split_into_words': False\n",
    "                   }\n",
    "token_labels = ['city', 'state', 'country', 'remote']\n",
    "label2id, id2label = create_ner_label_mappings(token_labels)\n",
    "\n",
    "train_batch = 4\n",
    "test_batch = 4\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddf1072-06f2-4495-9751-96191ac9c081",
   "metadata": {},
   "source": [
    "## Base Model and tokenizer Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "46b33078-b58c-47bc-834b-fc25b3366a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/connor/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/connor/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/connor/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/connor/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/connor/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-CITY\",\n",
      "    \"2\": \"I-CITY\",\n",
      "    \"3\": \"B-STATE\",\n",
      "    \"4\": \"I-STATE\",\n",
      "    \"5\": \"B-COUNTRY\",\n",
      "    \"6\": \"I-COUNTRY\",\n",
      "    \"7\": \"B-REMOTE\",\n",
      "    \"8\": \"I-REMOTE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"B-CITY\": 1,\n",
      "    \"B-COUNTRY\": 5,\n",
      "    \"B-REMOTE\": 7,\n",
      "    \"B-STATE\": 3,\n",
      "    \"I-CITY\": 2,\n",
      "    \"I-COUNTRY\": 6,\n",
      "    \"I-REMOTE\": 8,\n",
      "    \"I-STATE\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/connor/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/model.safetensors\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForTokenClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_funct = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "base_model = DistilBertForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=len(id2label.keys()), id2label=id2label, label2id=label2id\n",
    ")\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b44f57-ebf1-4c96-b2e2-69c2e3cd2c02",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3f5915f8-bc11-4104-acb4-bf62513e9611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 92/92 [00:00<00:00, 2690.12 examples/s]\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 2955.47 examples/s]\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/connor/anaconda3/envs/py3_10/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 92\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 115\n",
      "  Number of trainable parameters = 66,369,801\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [115/115 04:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Report</th>\n",
       "      <th>Pred Labels</th>\n",
       "      <th>True Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.995900</td>\n",
       "      <td>0.569134</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.694737</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "        CITY       0.48      0.70      0.57        20\n",
       "     COUNTRY       1.00      0.67      0.80        12\n",
       "      REMOTE       1.00      0.67      0.80         3\n",
       "       STATE       0.75      1.00      0.86         9\n",
       "\n",
       "   micro avg       0.65      0.75      0.69        44\n",
       "   macro avg       0.81      0.76      0.76        44\n",
       "weighted avg       0.71      0.75      0.71        44\n",
       "</td>\n",
       "      <td>[['B-COUNTRY', 'O', 'B-CITY', 'B-CITY'], ['O', 'O', 'O', 'O', 'B-CITY'], ['B-CITY', 'B-CITY', 'O', 'B-STATE'], ['B-CITY', 'B-CITY', 'B-CITY', 'B-CITY', 'O', 'B-STATE', 'O', 'B-CITY', 'O', 'B-STATE', 'O', 'B-STATE'], ['B-CITY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['O', 'O', 'B-REMOTE'], ['B-CITY', 'O', 'B-COUNTRY', 'B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE', 'O', 'B-STATE'], ['B-REMOTE'], ['B-CITY', 'B-CITY', 'O', 'B-STATE'], ['B-CITY', 'B-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'B-CITY', 'O', 'B-STATE', 'O'], ['B-CITY', 'O', 'O', 'B-STATE'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY']]</td>\n",
       "      <td>[['B-COUNTRY', 'O', 'B-CITY', 'I-CITY'], ['O', 'O', 'O', 'O', 'B-CITY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'I-CITY', 'I-CITY', 'I-CITY', 'O', 'B-STATE', 'O', 'B-CITY', 'O', 'B-STATE', 'O', 'B-REMOTE'], ['B-CITY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-COUNTRY', 'O', 'B-REMOTE'], ['B-CITY', 'O', 'B-COUNTRY', 'B-COUNTRY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE', 'O', 'B-COUNTRY'], ['B-REMOTE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE', 'O'], ['B-CITY', 'O', 'O', 'B-COUNTRY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.319600</td>\n",
       "      <td>0.263317</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.919540</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "        CITY       0.90      0.95      0.93        20\n",
       "     COUNTRY       1.00      0.83      0.91        12\n",
       "      REMOTE       1.00      0.67      0.80         3\n",
       "       STATE       0.90      1.00      0.95         9\n",
       "\n",
       "   micro avg       0.93      0.91      0.92        44\n",
       "   macro avg       0.95      0.86      0.90        44\n",
       "weighted avg       0.94      0.91      0.92        44\n",
       "</td>\n",
       "      <td>[['B-COUNTRY', 'O', 'B-CITY', 'I-CITY'], ['O', 'O', 'O', 'O', 'B-CITY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['O', 'B-CITY', 'I-CITY', 'I-CITY', 'O', 'B-STATE', 'O', 'B-CITY', 'O', 'B-STATE', 'O', 'O'], ['B-CITY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-COUNTRY', 'O', 'B-REMOTE'], ['B-CITY', 'O', 'B-COUNTRY', 'B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE', 'O', 'B-COUNTRY'], ['B-REMOTE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE', 'O'], ['B-CITY', 'O', 'O', 'B-STATE'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY']]</td>\n",
       "      <td>[['B-COUNTRY', 'O', 'B-CITY', 'I-CITY'], ['O', 'O', 'O', 'O', 'B-CITY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'I-CITY', 'I-CITY', 'I-CITY', 'O', 'B-STATE', 'O', 'B-CITY', 'O', 'B-STATE', 'O', 'B-REMOTE'], ['B-CITY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-COUNTRY', 'O', 'B-REMOTE'], ['B-CITY', 'O', 'B-COUNTRY', 'B-COUNTRY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE', 'O', 'B-COUNTRY'], ['B-REMOTE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE', 'O'], ['B-CITY', 'O', 'O', 'B-COUNTRY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.086500</td>\n",
       "      <td>0.173407</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "        CITY       0.90      0.95      0.93        20\n",
       "     COUNTRY       1.00      0.83      0.91        12\n",
       "      REMOTE       1.00      1.00      1.00         3\n",
       "       STATE       0.90      1.00      0.95         9\n",
       "\n",
       "   micro avg       0.93      0.93      0.93        44\n",
       "   macro avg       0.95      0.95      0.95        44\n",
       "weighted avg       0.94      0.93      0.93        44\n",
       "</td>\n",
       "      <td>[['B-COUNTRY', 'O', 'B-CITY', 'I-CITY'], ['O', 'O', 'O', 'O', 'B-CITY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['O', 'B-CITY', 'I-CITY', 'I-CITY', 'O', 'B-STATE', 'O', 'B-CITY', 'O', 'B-STATE', 'O', 'B-REMOTE'], ['B-CITY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-COUNTRY', 'O', 'B-REMOTE'], ['B-CITY', 'O', 'B-COUNTRY', 'B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE', 'O', 'B-COUNTRY'], ['B-REMOTE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE', 'O'], ['B-CITY', 'O', 'O', 'B-STATE'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY']]</td>\n",
       "      <td>[['B-COUNTRY', 'O', 'B-CITY', 'I-CITY'], ['O', 'O', 'O', 'O', 'B-CITY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'I-CITY', 'I-CITY', 'I-CITY', 'O', 'B-STATE', 'O', 'B-CITY', 'O', 'B-STATE', 'O', 'B-REMOTE'], ['B-CITY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-COUNTRY', 'O', 'B-REMOTE'], ['B-CITY', 'O', 'B-COUNTRY', 'B-COUNTRY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE', 'O', 'B-COUNTRY'], ['B-REMOTE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE', 'O'], ['B-CITY', 'O', 'O', 'B-COUNTRY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.168735</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "        CITY       0.90      0.95      0.93        20\n",
       "     COUNTRY       1.00      0.83      0.91        12\n",
       "      REMOTE       1.00      1.00      1.00         3\n",
       "       STATE       0.90      1.00      0.95         9\n",
       "\n",
       "   micro avg       0.93      0.93      0.93        44\n",
       "   macro avg       0.95      0.95      0.95        44\n",
       "weighted avg       0.94      0.93      0.93        44\n",
       "</td>\n",
       "      <td>[['B-COUNTRY', 'O', 'B-CITY', 'I-CITY'], ['O', 'O', 'O', 'O', 'B-CITY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['O', 'B-CITY', 'I-CITY', 'I-CITY', 'O', 'B-STATE', 'O', 'B-CITY', 'O', 'B-STATE', 'O', 'B-REMOTE'], ['B-CITY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-COUNTRY', 'O', 'B-REMOTE'], ['B-CITY', 'O', 'B-COUNTRY', 'B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE', 'O', 'B-COUNTRY'], ['B-REMOTE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE', 'O'], ['B-CITY', 'O', 'O', 'B-STATE'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY']]</td>\n",
       "      <td>[['B-COUNTRY', 'O', 'B-CITY', 'I-CITY'], ['O', 'O', 'O', 'O', 'B-CITY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'I-CITY', 'I-CITY', 'I-CITY', 'O', 'B-STATE', 'O', 'B-CITY', 'O', 'B-STATE', 'O', 'B-REMOTE'], ['B-CITY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-COUNTRY', 'O', 'B-REMOTE'], ['B-CITY', 'O', 'B-COUNTRY', 'B-COUNTRY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE', 'O', 'B-COUNTRY'], ['B-REMOTE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE', 'O'], ['B-CITY', 'O', 'O', 'B-COUNTRY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.158870</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "        CITY       0.90      0.95      0.93        20\n",
       "     COUNTRY       1.00      0.83      0.91        12\n",
       "      REMOTE       1.00      1.00      1.00         3\n",
       "       STATE       0.90      1.00      0.95         9\n",
       "\n",
       "   micro avg       0.93      0.93      0.93        44\n",
       "   macro avg       0.95      0.95      0.95        44\n",
       "weighted avg       0.94      0.93      0.93        44\n",
       "</td>\n",
       "      <td>[['B-COUNTRY', 'O', 'B-CITY', 'I-CITY'], ['O', 'O', 'O', 'O', 'B-CITY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['O', 'B-CITY', 'I-CITY', 'I-CITY', 'O', 'B-STATE', 'O', 'B-CITY', 'O', 'B-STATE', 'O', 'B-REMOTE'], ['B-CITY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-COUNTRY', 'O', 'B-REMOTE'], ['B-CITY', 'O', 'B-COUNTRY', 'B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE', 'O', 'B-COUNTRY'], ['B-REMOTE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE', 'O'], ['B-CITY', 'O', 'O', 'B-STATE'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY']]</td>\n",
       "      <td>[['B-COUNTRY', 'O', 'B-CITY', 'I-CITY'], ['O', 'O', 'O', 'O', 'B-CITY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'I-CITY', 'I-CITY', 'I-CITY', 'O', 'B-STATE', 'O', 'B-CITY', 'O', 'B-STATE', 'O', 'B-REMOTE'], ['B-CITY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-COUNTRY', 'O', 'B-REMOTE'], ['B-CITY', 'O', 'B-COUNTRY', 'B-COUNTRY', 'O', 'B-COUNTRY'], ['B-CITY', 'O', 'B-COUNTRY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE', 'O', 'B-COUNTRY'], ['B-REMOTE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'I-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY'], ['B-CITY', 'I-CITY', 'O', 'B-STATE', 'O'], ['B-CITY', 'O', 'O', 'B-COUNTRY'], ['O', 'O'], ['B-CITY', 'O', 'B-STATE'], ['B-CITY', 'O', 'B-COUNTRY']]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./location_bert/checkpoint-23\n",
      "Configuration saved in ./location_bert/checkpoint-23/config.json\n",
      "Model weights saved in ./location_bert/checkpoint-23/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./location_bert/checkpoint-46\n",
      "Configuration saved in ./location_bert/checkpoint-46/config.json\n",
      "Model weights saved in ./location_bert/checkpoint-46/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./location_bert/checkpoint-69\n",
      "Configuration saved in ./location_bert/checkpoint-69/config.json\n",
      "Model weights saved in ./location_bert/checkpoint-69/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./location_bert/checkpoint-92\n",
      "Configuration saved in ./location_bert/checkpoint-92/config.json\n",
      "Model weights saved in ./location_bert/checkpoint-92/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./location_bert/checkpoint-115\n",
      "Configuration saved in ./location_bert/checkpoint-115/config.json\n",
      "Model weights saved in ./location_bert/checkpoint-115/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./location_bert/checkpoint-115 (score: 0.15887023508548737).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./location_bert\n",
      "Configuration saved in ./location_bert/config.json\n",
      "Model weights saved in ./location_bert/model.safetensors\n"
     ]
    }
   ],
   "source": [
    "# Add the correct agurements to align_labels_with_tokens_batched function \n",
    "preprocess_funct_batched = partial(align_labels_with_tokens_batched, tokenizer=tokenizer_funct, label2id=label2id, tokenizer_config=tokenizer_config)\n",
    "# Train the model \n",
    "results = run_model_v10(train_rows, test_rows, preprocess_funct_batched, tokenizer_funct, tokenizer_config, epochs, train_batch, test_batch, base_model, out_put_dir, compute_metrics_token_classification,  True)\n",
    "del base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9f6df8de-e58a-4704-902e-60211201d896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1730087250471115,\n",
       " 'eval_precision': 0.9545454545454546,\n",
       " 'eval_recall': 0.9545454545454546,\n",
       " 'eval_f1': 0.9545454545454546,\n",
       " 'eval_report': '              precision    recall  f1-score   support\\n\\n        CITY       0.90      0.95      0.93        20\\n     COUNTRY       1.00      0.92      0.96        12\\n      REMOTE       1.00      1.00      1.00         3\\n       STATE       1.00      1.00      1.00         9\\n\\n   micro avg       0.95      0.95      0.95        44\\n   macro avg       0.98      0.97      0.97        44\\nweighted avg       0.96      0.95      0.95        44\\n',\n",
       " 'eval_runtime': 0.1048,\n",
       " 'eval_samples_per_second': 228.978,\n",
       " 'eval_steps_per_second': 57.244,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55502714-361c-4786-804e-9f1b0c2d6147",
   "metadata": {},
   "source": [
    "# Model Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0bb3d4-be10-4e08-bd36-9eace1982220",
   "metadata": {},
   "source": [
    "## Load Existing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcfefe36-15d4-447a-b380-cd0a3a43bdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "token_classifier = AutoModelForTokenClassification.from_pretrained(out_put_dir)\n",
    "token_classifier.to(device)\n",
    "tokenizer_funct = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ea8b2d-f618-4a06-be27-503f7cffefe4",
   "metadata": {},
   "source": [
    "## Eval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9efac8d0-7b32-4101-ad7e-ff8b2790a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ner_example(example_inputs, classifier):\n",
    "    with torch.no_grad():\n",
    "        outputs = classifier(**example_inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def make_predictions(input_example, token_classifier, tokenizer, id2label):\n",
    "    # move input_tensors onto Device\n",
    "    inputs = {k: v.to(device) for k, v in input_example.items()} \n",
    "    # Make prediction\n",
    "    result = eval_ner_example(inputs, token_classifier) # returns a tensor\n",
    "    # Move prediction back to cpu\n",
    "    predictions_np = result.cpu().numpy() # returns a nparray\n",
    "    # Move attention_mask back to cpu\n",
    "    attention_mask_np = input_example['attention_mask'].cpu().numpy()# returns a nparray\n",
    "    # Replace the 1s in the attention mask with the actual returned prediction. replace the 0s with -100\n",
    "    adjusted_predictions = np.where(attention_mask_np == 1, predictions_np, -100) \n",
    "    flat_predictions = adjusted_predictions.flatten() # remove the extra dimension  \n",
    "    \n",
    "    attention_mask_flat = attention_mask_np.flatten() # remove extra dimension on np array\n",
    "    attention_mask_list = attention_mask_flat.tolist() # convert nparray to list \n",
    "    last_real_token_index = len(attention_mask_list) - 1 - attention_mask_list[::-1].index(1) # grab the index of the last real item\n",
    "    filtered_predictions_ids = flat_predictions[:last_real_token_index+1] # use last index +1 to remove the -100s\n",
    "    \n",
    "    pred_label_names = [id2label[id] for id in filtered_predictions_ids] # Convert Ids to labels \n",
    "    inputs = {k: v.to('cpu') for k, v in inputs.items()} \n",
    "    input_ids_list = inputs['input_ids'].squeeze().tolist()  # Remove extra dimension and convert tensor to list  \n",
    "    # Grab the tokens for the input example then conver them from ids to actual string token. then Remove the pad tokens. \n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids_list) \n",
    "    tokens = [token for token in tokens if token != '[PAD]']\n",
    "    \n",
    "    del inputs # Free up memory  \n",
    "    return pred_label_names, tokens, last_real_token_index\n",
    "    \n",
    "def eval_example_with_true_label(example, token_classifier, tokenizer, tokenizer_config, label2id, id2label, device):\n",
    "    input_example = align_labels_with_tokens_single(example, tokenizer=tokenizer, label2id=label2id, tokenizer_config={'return_tensors':\"pt\", **tokenizer_config})\n",
    "    true_label_ids = input_example.pop('labels') # the true labels for an example\n",
    "\n",
    "    pred_label_names, tokens, last_real_token_index = make_predictions(input_example, token_classifier, tokenizer, id2label)\n",
    "    \n",
    "    # Grab the actual label name from the true labels\n",
    "    true_label_names = [-100 if id == -100 else id2label[id] for id in true_label_ids[:last_real_token_index+1]] # Convert Ids to labels\n",
    "    \n",
    "    print(f\"Raw Location: \\n{example['text']}\\n\")\n",
    "    print(f\"{'Pred':<15}{'True':<15}{'Token':<10}\")\n",
    "    for pred, actual, token in zip(pred_label_names, true_label_names, tokens):\n",
    "        print(f\"{pred:<15}{actual:<15}{token:<10}\")\n",
    "    \n",
    "    # Free up memory \n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache() \n",
    "    \n",
    "    return {\n",
    "        'predicted_labels': pred_label_names, \n",
    "        'actual_labels': true_label_names, \n",
    "        'tokens': tokens,\n",
    "        'raw_string': example['text']\n",
    "    }\n",
    "\n",
    "def eval_example_no_label(example, token_classifier, tokenizer, tokenizer_config, label2id, id2label, device):\n",
    "    input_example = tokenizer_funct(example, return_tensors=\"pt\", **tokenizer_config)\n",
    "    input_example.pop(\"offset_mapping\", None)\n",
    "\n",
    "    pred_label_names, tokens, _ = make_predictions(input_example, token_classifier, tokenizer, id2label)\n",
    "        \n",
    "    print(f\"Raw Location: \\n{example}\\n\")\n",
    "    print(f\"{'Pred':<15}{'Token':<10}\")\n",
    "    for pred, token in zip(pred_label_names, tokens):\n",
    "        print(f\"{pred:<15}{token:<10}\")\n",
    "    \n",
    "    # Free up memory  \n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache() \n",
    "    \n",
    "    return {\n",
    "        'predicted_labels': pred_label_names, \n",
    "        'tokens': tokens,\n",
    "        'raw_string': example\n",
    "    }\n",
    "\n",
    "def check_test_preds(df, token_classifier, tokenizer_funct, tokenizer_config, label2id, id2label, device):\n",
    "    should_break_loop = False\n",
    "    for row_count, row in df.iterrows():\n",
    "        obj_w_label = eval_example_with_true_label(row, token_classifier, tokenizer_funct, tokenizer_config, label2id, id2label, device)\n",
    "        while True:\n",
    "            print()\n",
    "            user_input = input(\" F to continue. P Escape\")\n",
    "            user_input = user_input.lower() \n",
    "            if user_input in ['f','p']:\n",
    "                if user_input == 'p':\n",
    "                    should_break_loop = True\n",
    "                    break\n",
    "                elif user_input == 'f': \n",
    "                    break\n",
    "\n",
    "        clear_output()\n",
    "        if should_break_loop :\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cd3d5-383b-4317-adc1-7252f8d510b3",
   "metadata": {},
   "source": [
    "### Testing Eval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "329e8329-99b2-4926-9367-2d90f10cc105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                                    us - san francisco\n",
       "label    [{'end': 18, 'labels': ['city'], 'start': 5, '...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = test_rows.iloc[0]\n",
    "# example = new_df.iloc[0]['fmt_raw_location']\n",
    "example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5d04ee7f-924c-40b4-b4b4-74198effdf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Location: \n",
      "us - san francisco\n",
      "\n",
      "Pred           True           Token     \n",
      "O              -100           [CLS]     \n",
      "B-COUNTRY      B-COUNTRY      us        \n",
      "O              O              -         \n",
      "B-CITY         B-CITY         san       \n",
      "I-CITY         I-CITY         francisco \n",
      "I-REMOTE       -100           [SEP]     \n",
      "\n",
      "Raw Location: \n",
      "us - san francisco\n",
      "\n",
      "Pred           Token     \n",
      "O              [CLS]     \n",
      "B-COUNTRY      us        \n",
      "O              -         \n",
      "B-CITY         san       \n",
      "I-CITY         francisco \n",
      "I-REMOTE       [SEP]     \n"
     ]
    }
   ],
   "source": [
    "obj_w_label = eval_example_with_true_label(example, token_classifier, tokenizer_funct, tokenizer_config, label2id, id2label, device)\n",
    "print()\n",
    "obj_no_label = eval_example_no_label(example['text'], token_classifier, tokenizer_funct, tokenizer_config, label2id, id2label, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d80b06-49b7-4805-9f80-d02df1b180ed",
   "metadata": {},
   "source": [
    "## Checking Test Rows predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7bfcf2eb-03b0-49d7-ba10-a9a8a6dd2ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test_preds(test_rows, token_classifier, tokenizer_funct, tokenizer_config, label2id, id2label, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7dfc0f-8959-40dc-9fe3-72999bef9181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a27f9e7a-6532-4586-a9b0-9c0bc1e5b02d",
   "metadata": {},
   "source": [
    "# Sand Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8a139f82-da96-4650-ad53-851ed86e9dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2149, 1011, 2624, 3799, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 5, 0, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_example = align_labels_with_tokens_single(example, tokenizer=tokenizer_funct, label2id=label2id, tokenizer_config={**tokenizer_config})\n",
    "input_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1678b-3a5a-4bd1-9538-a597e981c724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ebfadc3f-4cbb-424b-9c02-15ad6cdd4a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                                    us - san francisco\n",
       "label    [{'end': 18, 'labels': ['city'], 'start': 5, '...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0a993700-ce20-431f-bbf3-46dbc65f640e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 6556, 102, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 7, -100, -100, -100, -100, -100, -100, -100, -100]}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_labels_with_tokens_single(example, tokenizer=tokenizer_funct, label2id=label2id, tokenizer_config=tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "834337d0-4fbb-424d-bed0-2998c2feda7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'end': 18, 'labels': ['city'], 'start': 5, 'text': 'san francisco'}, {'end': 2, 'labels': ['country'], 'start': 0, 'text': 'us'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2149, 1011, 2624, 3799, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 5, 0, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_labels_with_tokens_single(example, tokenizer=tokenizer_funct, label2id=label2id, tokenizer_config=tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc6f845-8d6c-4f84-8bc6-0c860aee036e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcddb3a-7001-4523-908e-1de8e28ad73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89018dd7-bfbb-42f0-af18-3add46eef00a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Old Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1fdd92bc-b173-427a-b090-f2aa3a2b893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_classification(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "def build_confusion(y_true, y_pred, classes):\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    make_confusion_matrix(cf_matrix, categories=classes, figsize=(8,6), cbar=False)\n",
    "\n",
    "def eval_model(test_rows, classifier, id2label, token_max):\n",
    "    tokenizer_args= {'truncation':True, 'max_length':token_max, 'padding':True}\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for _, row in test_rows.iterrows():\n",
    "        y_pred.append(classifier(preprocess_text(row['text']), **tokenizer_args)[0]['label'])\n",
    "        y_true.append(id2label[row['label']])  \n",
    "    return y_true, y_pred\n",
    "\n",
    "# def eval_example(example, tokenizer_args, classifier):\n",
    "#     val = classifier(preprocess_text(example), **tokenizer_args)\n",
    "#     return val\n",
    "\n",
    "def calc_accuracy(y_true,y_pred):\n",
    "    accuracy = 0\n",
    "    for x in range(len(y_true)):\n",
    "        if y_pred[x]==y_true[x]:\n",
    "            accuracy+=1\n",
    "    print(f'Accuracy: {(accuracy/len(y_true))*100:.2f}%')\n",
    "    \n",
    "def make_confusion_matrix(cf,\n",
    "                          categories='auto',\n",
    "                          cbar=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "    \n",
    "    group_labels = ['' for i in range(cf.size)]\n",
    "    group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    \n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "    accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "    stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    \n",
    "    if figsize==None:\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    sn.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label' + stats_text)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "def print_GPU():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "def format_rows_for_train(df, label2id, preprocess_text=None):\n",
    "    newdf = df.loc[df['label'].notnull()].copy()\n",
    "    if preprocess_text:\n",
    "        newdf['description'] = newdf['description'].apply(lambda x: preprocess_text(x))\n",
    "    \n",
    "    newdf['text'] = newdf[['title', 'description']].apply(lambda x: f\"title {x['title'].lower()} description {x['description']}\", axis=1)\n",
    "    newdf['label'] = newdf['label'].apply(lambda x: label2id[x])\n",
    "    \n",
    "    final_df = newdf[['text','label']].copy()\n",
    "    training_rows_DS = Dataset.from_pandas(final_df)\n",
    "    training_rows_DS = training_rows_DS.train_test_split(train_size=.8, seed=42) \n",
    "\n",
    "    train_rows = pd.DataFrame(list(zip(training_rows_DS['train']['text'], training_rows_DS['train']['label'])), columns =['text','label'])\n",
    "    test_rows = pd.DataFrame(list(zip(training_rows_DS['test']['text'], training_rows_DS['test']['label'])), columns =['text','label'])\n",
    "    \n",
    "    return training_rows_DS, train_rows, test_rows\n",
    "\n",
    "def print_label_counts(split_datasets, label_column='label'):\n",
    "    for split in ['train', 'test']:\n",
    "        dataset = split_datasets[split] \n",
    "        labels = dataset[label_column]  \n",
    "        \n",
    "        if isinstance(labels, pd.Series):\n",
    "            label_counts = labels.value_counts()\n",
    "        else:\n",
    "            label_counts = pd.Series(labels).value_counts()\n",
    "        \n",
    "        print(f\"Label counts for {split} set:\")\n",
    "        print_string = ''\n",
    "        for idx,[key,value] in enumerate(label_counts.items()):\n",
    "            print_string += f\"{key}: {value} \"\n",
    "            if idx != len(label_counts.keys())-1:\n",
    "                print_string += \"| \"\n",
    "            \n",
    "        print(print_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f63dcb-d073-46e1-bc78-dbb37d5264c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Old Data Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90495a5f-815b-4659-95ec-bcede0d3519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_df = pd.read_csv('MASTER_DF.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9532b0-ef3b-4934-b156-90fc2abb87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df522adf-b14a-4435-9f42-b5af66de9671",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = e_df.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98859892-ba5a-44aa-9ebe-e2318526b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_token_count(text, tokenizer_funct):\n",
    "    encoded_dict = tokenizer_funct.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True, \n",
    "        max_length=512,  \n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,  \n",
    "    )\n",
    "    total_token= 0\n",
    "    for x in range(len(encoded_dict['input_ids'])):\n",
    "        total_token += len(encoded_dict['input_ids'][x])\n",
    "        if x !=0:\n",
    "            total_token-=2\n",
    "    \n",
    "    return total_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b657a2-ae1f-4af9-a16a-01a73ca5e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_df['tk_count_description'] = e_df['description'].apply(lambda x: calculate_total_token_count(x, tokenizer_funct))\n",
    "e_df['tk_count_title'] = e_df['title'].apply(lambda x: calculate_total_token_count(x.lower(), tokenizer_funct))\n",
    "e_df['processed_description'] = e_df['description'].apply(lambda x: preprocess_text(x))\n",
    "e_df['tk_count_pre_description'] = e_df['processed_description'].apply(lambda x: calculate_total_token_count(x, tokenizer_funct))\n",
    "\n",
    "e_df['fmt_text'] = e_df[['title', 'processed_description']].apply(lambda x: f\"title {x['title'].lower()} description {x['processed_description']}\", axis=1)\n",
    "\n",
    "e_df['tk_count_fmt_text'] = e_df['fmt_text'].apply(lambda x: calculate_total_token_count(x, tokenizer_funct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab4c0e-b539-44dc-9457-74d45686df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_df(df):\n",
    "\n",
    "    mean_token_fmt = df['tk_count_fmt_text'].mean()    \n",
    "    max_token_fmt = df['tk_count_fmt_text'].max()\n",
    "    \n",
    "    mean_token_title = df['tk_count_title'].mean()    \n",
    "    max_token_title = df['tk_count_title'].max()\n",
    "    \n",
    "    mean_token_p_description = df['tk_count_pre_description'].mean()    \n",
    "    max_token_p_description = df['tk_count_pre_description'].max()\n",
    "    \n",
    "    count_fmt_rows_above_max_length = (df['tk_count_fmt_text'] > 512).sum()\n",
    "    count_title_rows_above_mean = (df['tk_count_title'] > int(mean_token_title)).sum()\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f\"Full text: Mean: {mean_token_fmt}, Max: {max_token_fmt}\")\n",
    "    print(f\"Title text: Mean: {mean_token_title}, Max: {max_token_title}\")\n",
    "    print(f\"Processed Description text: Mean: {mean_token_p_description}, Max: {max_token_p_description}\")\n",
    "    \n",
    "    print(f\"Title above mean title size: {count_title_rows_above_mean}, or {(count_title_rows_above_mean/len(df))*100:.2f}%\")\n",
    "    print(f\"Processed Description text above 512: {count_fmt_rows_above_max_length}, or {(count_fmt_rows_above_max_length/len(df))*100:.2f}%\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb56bf6-8ee8-4587-9bd0-a2e9bb81a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_df(e_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86470c-556e-4c71-a502-840c0ebcccfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c70a65b-8829-4866-8ec8-5f002d68470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[{\"start\": 1909, \"end\": 1917, \"text\": \"2+ years\", \"labels\": [\"exp\"]}, \n",
    " {\"start\": 2035, \"end\": 2043, \"text\": \"3+ years\", \"labels\": [\"exp\"]}, \n",
    " {\"start\": 2605, \"end\": 2613, \"text\": \"1+ years\", \"labels\": [\"exp\"]}, \n",
    " {\"start\": 1703, \"end\": 1708, \"text\": \"Azure\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 1958, \"end\": 1965, \"text\": \"Windows\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 1973, \"end\": 1978, \"text\": \"Linux\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 1735, \"end\": 1750, \"text\": \"CI/CI pipelines\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 2522, \"end\": 2532, \"text\": \"PowerShell\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 2534, \"end\": 2538, \"text\": \"Bash\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 2540, \"end\": 2546, \"text\": \"Python\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 2548, \"end\": 2554, \"text\": \"Groovy\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 2377, \"end\": 2386, \"text\": \"Terraform\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 2388, \"end\": 2395, \"text\": \"Ansible\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 2397, \"end\": 2408, \"text\": \"Chef/Puppet\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 2410, \"end\": 2416, \"text\": \"Docker\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 2424, \"end\": 2434, \"text\": \"Kubernetes\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 2198, \"end\": 2210, \"text\": \"Azure DevOps\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 2223, \"end\": 2230, \"text\": \"Jenkins\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 2239, \"end\": 2245, \"text\": \"Bamboo\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 2642, \"end\": 2657, \"text\": \"Microsoft Azure\", \"labels\": [\"technology\"]}, \n",
    " {\"start\": 2726, \"end\": 2729, \"text\": \"AWS\", \"labels\": [\"technology\"]}, \n",
    "{\"start\": 2733, \"end\": 2736, \"text\": \"GCP\", \"labels\": [\"technology\"]}, \n",
    "{\"start\": 3078, \"end\": 3101, \"text\": \"Microsoft Certification\", \"labels\": [\"degree\"]}]\n",
    "\n",
    "\n",
    "\n",
    "Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you! At 3Cloud, we hire people who aren’t afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud’s core values because they know that it will result in amazing experiences and solutions for our clients and each other. As a DevOps Engineer, your primary responsibility will be to implement DevOps solutions in Azure. The role is a technical, consultative, and client facing role that will be accountable for client solution development, delivery and support. The ideal candidate will have experience in consulting and have demonstrated success in technical implementations and project work. Key Responsibilities  As a DevOps Engineer you will:  Participate in technical envisioning, technical design, and delivery of assigned projects. Work with 3Cloud Architects to support project efforts from a technical perspective. Execute the implementation of designed solutions into client deliverables. Assist with design and deployment of client workloads into Azure. Providing technical expertise and support across the following four areas of specialization:  Datacenter Transformation Azure Infrastructure DevOps and CI/CI pipelines Cloud automation      Be an essential part of the team responsible for crafting and delivering creative cloud solutions for our clients. Experience Required  2+ years of experience in system administration (Windows and/or Linux), DevOps and/or software development and IT Operations. 3+ years project experience migrating and deploying cloud-based solutions. Hands-on implementation experience of Continuous Integration and Continuous Delivery in Azure DevOps (VSTS/TFS), Jenkins, and/or Bamboo (or another similar tool) Experience with various cloud deployment methodologies. Hands-on experience with some of the following:  Terraform, Ansible, Chef/Puppet, Docker and/or Kubernetes is preferred, but not required. Automation experience - very good scripting knowledge (PowerShell, Bash, Python, Groovy, etc.).  Experience can be from any public cloud. 1+ years implementing and supporting Microsoft Azure infrastructure and topologies (or demonstrated experience form with AWS or GCP) Understanding of cloud ecosystem and leading-edge cloud emerging technologies. Performance analysis, troubleshooting and remediation techniques. Strong analytical problem-solving ability. Strong presentation, written and verbal communication skills. Self-starter with the ability to work independently or as part of a virtual project team. Microsoft Certification a plus.  Don’t meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. At this time, we cannot sponsor applicants for work visas.,33,DevOps Engineer,2024-04-09T02:49:25.226453Z,Remote - Philippines,340.176,2,2024-04-09T02:49:25.226486Z,3cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b6436-b0ea-4007-bc26-bdfb229a16cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c32b9-1097-4114-96a4-04695e99b8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
